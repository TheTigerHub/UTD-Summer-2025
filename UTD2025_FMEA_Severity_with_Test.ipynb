{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tigerzhao0/UTD-Summer-2025/blob/main/UTD2025_FMEA_Severity_with_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgwOMXjswnBk",
        "outputId": "a2088072-086e-4b01-805e-0282cce985d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "installing hugging face libraries for QLoRA\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m110.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hinstalled accelerate\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hinstalled peft\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hinstalled bitsandbytes\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m118.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hinstalled transformers\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0minstalled datasets\n"
          ]
        }
      ],
      "source": [
        "print (\"installing hugging face libraries for QLoRA\")\n",
        "\n",
        "!pip install -q accelerate==0.30.1 #hardware acceleration\n",
        "print (\"installed accelerate\")\n",
        "!pip install -q peft==0.11.1 #peft is parameter effecient fine tuning which allows LoRA and QLoRA\n",
        "print (\"installed peft\")\n",
        "!pip install -q bitsandbytes #bits and bytes helps with quantization\n",
        "print(\"installed bitsandbytes\")\n",
        "!pip install -q transformers==4.41.1 #hugging face library for pre-trained models, tokenizers\n",
        "print(\"installed transformers\")\n",
        "!pip install -q datasets==2.19.0 #datasets is used for loading, processing, and managing datasets\n",
        "print(\"installed datasets\")\n",
        "!pip install -q scikit-learn==1.4.2 # scikit-learn for data splitting\n",
        "print(\"installed scikit-learn\")\n",
        "!pip install -q pandas #used for data manipulation and analysis\n",
        "print(\"installed pandas\")\n",
        "!pip install -q openpyxl #read excel\n",
        "print(\"installed openpyxl\")\n",
        "\n",
        "print (\"\\n libraries installed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EI5WZYrHKSOI",
        "outputId": "545b93c1-d6df-4fbe-b42d-df9089cacaa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "installing hugging face libraries for QLoRA\n",
            "\u001b[33mWARNING: peft 0.15.2 does not provide the extra 'bnb'\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.1/411.1 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.3/365.3 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m131.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\n",
            " libraries installed\n",
            "\n",
            " restart runtime\n"
          ]
        }
      ],
      "source": [
        "print (\"installing hugging face libraries for QLoRA\")\n",
        "\n",
        "!pip install -q \"peft[bnb]\" --upgrade #installs peft pacakge with bnb extra, integrates peft and bnb\n",
        "\n",
        "!pip install -q accelerate bitsandbytes \"transformers[torch]\" --upgrade #installs transformers with dependencies for PyTorch\n",
        "\n",
        "!pip install -q datasets==2.19.0 scikit-learn==1.4.2 pandas openpyxl #installs datasets, scikit-learn, pandas, and openpyxl\n",
        "\n",
        "print (\"\\n libraries installed\")\n",
        "print (\"\\n restart runtime\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "W8OzvxnGNf10",
        "outputId": "bd2e059f-a70e-4520-e2f1-b5ef04d72179"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "id2label mapping: {0: '1', 1: '2', 2: '3', 3: '4', 4: '5', 5: '6', 6: '7', 7: '8', 8: '9', 9: '10'}\n",
            "label2id mapping: {'1': 0, '2': 1, '3': 2, '4': 3, '5': 4, '6': 5, '7': 6, '8': 7, '9': 8, '10': 9}\n",
            "Compute dtype set to bfloat16 for Ampere+ GPU.\n"
          ]
        }
      ],
      "source": [
        "#imports libraries and configures data and training\n",
        "\n",
        "import torch #torch is necessary for the Llama model\n",
        "import pandas as pd #will be used later to load, clean, and process data before converting to a hugginf face dataset\n",
        "import numpy as np #library for large, multi dimensional arrays and matricies as well as operations on them\n",
        "from datasets import Dataset, DatasetDict #provides effecient way to handle large datasets for machine learning, espicially natural language processing\n",
        "from sklearn.model_selection import train_test_split #from sci-kit learn library, splits dataset into training and validation\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report #these provide information on the accuracy of the model\n",
        "from transformers import ( #key components from transformers\n",
        "    AutoTokenizer, #loads correct tokenize automatically, tokenizers convert text into numerical IDs\n",
        "    AutoModelForSequenceClassification, #standard class the automatically load the correct model for sequence classification tasks\n",
        "    TrainingArguments, #class to configure training\n",
        "    Trainer, #class that simplifies training and eval loop for hugging face models\n",
        "    DataCollatorWithPadding, #pads sequences of data in a batch to be the same legnth\n",
        "    BitsAndBytesConfig # manual QLoRA config, quantization is important bc of GPU resources\n",
        ")\n",
        "from peft import (\n",
        "    get_peft_model, #wraps model with peft adapter like QLoRA\n",
        "    LoraConfig, #class for LoRA config\n",
        "    TaskType, #specifies task type\n",
        "    prepare_model_for_kbit_training # manual QLoRA setup with kbit quantization\n",
        ")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\") #makes output cleaner\n",
        "from huggingface_hub import notebook_login # Keep login for Llama 3.1\n",
        "\n",
        "CSV_PATH = \"2.4.xlsx\" #uploaded excel file\n",
        "\n",
        "# Column Names (Ensure these EXACTLY match your cleaned CSV/Excel headers)\n",
        "COL_SUBFUNCTION = \"Subfunction\"\n",
        "COL_REQUIREMENTS = \"Requirements\"\n",
        "COL_FAILURE_MODE = \"Potential Failure Mode and descriptions\" # Base name, will be cleane\n",
        "COL_EFFECT_PRIMARY = \"Potential Effect(s) of Failure (primary)\" # Base name\n",
        "COL_EFFECT_SECONDARY = \"Potential Effect(s) of Failure (secondary)\" # Base name\n",
        "COL_SEVERITY = \"Severity\" # target column\n",
        "\n",
        "# Input/Output Columns\n",
        "INPUT_TEXT_COLS = [\n",
        "    COL_SUBFUNCTION, COL_REQUIREMENTS, COL_FAILURE_MODE,\n",
        "    COL_EFFECT_PRIMARY, COL_EFFECT_SECONDARY\n",
        "]\n",
        "\n",
        "\n",
        "#need to fill in empty cells\n",
        "COLS_TO_FORWARD_FILL = [\n",
        "    COL_SUBFUNCTION, COL_REQUIREMENTS, COL_FAILURE_MODE\n",
        "]\n",
        "\n",
        "#want to predict this\n",
        "TARGET_COLUMN = COL_SEVERITY\n",
        "NUM_LABELS = 10 # Severity can only be 1-10\n",
        "\n",
        "# Model Config\n",
        "MODEL_NAME = \"meta-llama/Meta-Llama-3.1-8B-Instruct\" # using 3.2-1B bc 3.1 access has not been approved\n",
        "MAX_SEQ_LENGTH = 512 #for memory management\n",
        "\n",
        "# Training Config\n",
        "OUTPUT_DIR = \"fmea_severity_classifier_llama31_8b_standard_qlora\" # <<< New output dir name\n",
        "LEARNING_RATE = 1e-4      # common QLoRA starting point\n",
        "NUM_EPOCHS = 3            # Train for 3 epochs which is standard for fine tuning\n",
        "# MAX_STEPS = 500         # Alternative to epochs\n",
        "BATCH_SIZE_PER_DEVICE = 1 # may use 4 because model is smaller\n",
        "GRAD_ACCUMULATION_STEPS = 16 # effective batch size of 16\n",
        "LORA_R = 16 #the rank for LoRA matricies 16 is common\n",
        "LORA_ALPHA = 32 #scaling factor for LoRA updates twice or equal to LORA_R is standard\n",
        "LORA_DROPOUT = 0.05 #used for regularization, 0.05 is typical value\n",
        "LOGGING_STEPS = 10 #frequent updates\n",
        "SAVE_STRATEGY = \"epoch\" #when model should be saved\n",
        "EVAL_STRATEGY = \"epoch\"   #when eval should be preformed\n",
        "# -----done configing LoRA------\n",
        "\n",
        "\n",
        "#setup device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #checks if device is uisng cuda GPU, cpu for backup\n",
        "print(f\"Using device: {device}\") #prints out what device is being used\n",
        "if device.type == 'cpu': print(\"Warning: Running on CPU!\") #warning for if using cpu bc cpu training is way too slow\n",
        "\n",
        "# Setup Label Mappings\n",
        "labels_list = [str(i) for i in range(1, 11)] #creates list of strings for severity label from \"1\" to \"10\"\n",
        "id2label = {i: label for i, label in enumerate(labels_list)} #maps integers 0-9 to strings \"1\"-\"10\" needed for classification head and output interpretation\n",
        "label2id = {label: i for i, label in enumerate(labels_list)} #reverse of previous line, maps integers \"1\"-\"10\" to integers 0-9 which is required for training\n",
        "print(f\"id2label mapping: {id2label}\") #print for verification\n",
        "print(f\"label2id mapping: {label2id}\") #print for verification\n",
        "\n",
        "# Check GPU capability for compute dtype in BNBConfig will  use float16 because t4 is the free avalible one\n",
        "compute_dtype = torch.float16\n",
        "if torch.cuda.is_available():\n",
        "    if torch.cuda.get_device_capability()[0] >= 8: # Ampere+ (A100)\n",
        "        compute_dtype = torch.bfloat16\n",
        "        print(\"Compute dtype set to bfloat16 for Ampere+ GPU.\")\n",
        "    else:\n",
        "        print(\"Compute dtype set to float16.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jsvggni9DkTI",
        "outputId": "b743f962-cd4e-4067-db75-fd1520735d7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data from '2.4.xlsx'...\n",
            "❌ Error loading data: [Errno 2] No such file or directory: '2.4.xlsx'\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '2.4.xlsx'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4-1878403694.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCSV_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCSV_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         io = ExcelFile(\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1551\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '2.4.xlsx'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4-1878403694.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCSV_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCSV_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;31m#this line will perform string operations to datafram column names, removes new line characters with \\n, replaces multiple spaces with a single space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m#removes leading and trailing whitespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '2.4.xlsx'"
          ]
        }
      ],
      "source": [
        "#load and Preprocess Data\n",
        "#read Excel/CSV, clean header, forward filling, combine text features into a combined \"text\" column, and \"label\" column 0-9\n",
        "#splits data\n",
        "#converts to DatasetDict\n",
        "#normalization off by default\n",
        "\n",
        "print(f\"Loading data from '{CSV_PATH}'...\")\n",
        "\n",
        "\n",
        "#try except block handles reading from excel and csv, and then stores it into pandas dataframe, df\n",
        "try:\n",
        "    try: df = pd.read_excel(CSV_PATH)\n",
        "    except Exception: df = pd.read_csv(CSV_PATH)\n",
        "    #this line will perform string operations to datafram column names, removes new line characters with \\n, replaces multiple spaces with a single space\n",
        "    #removes leading and trailing whitespace\n",
        "    #saves the processed columns to cleaned_columns list\n",
        "    original_columns = df.columns.tolist(); df.columns = df.columns.str.replace('\\n', '', regex=False).str.replace(' +', ' ', regex=True).str.strip(); cleaned_columns = df.columns.tolist()\n",
        "    #maps cleaned columns to original columns\n",
        "    column_map = {clean: orig for clean, orig in zip(cleaned_columns, original_columns)}; print(f\"✅ Loaded {len(df)} rows. Cleaned columns: {cleaned_columns}\")\n",
        "except Exception as e: print(f\"❌ Error loading data: {e}\"); raise #print error if reading from excel or csv produced an exception\n",
        "\n",
        "\n",
        "# Function to get cleaned name robustly (optional, can hardcode if sure)\n",
        "def get_cleaned_name(config_name, df_cols, original_map): # Pass original map too\n",
        "    # Use split() and join() to collapse multiple spaces and remove newlines/strip\n",
        "    cleaned = ' '.join(str(config_name).replace('\\n', '').strip().split()) # same cleaning as before\n",
        "    if cleaned not in df_cols:\n",
        "         original_name = original_map.get(cleaned, config_name) # Try lookup original name if clean fails\n",
        "         print(f\"   Warning: Configured column '{config_name}' -> '{cleaned}' not found after cleaning. Check CSV/Excel headers and config variables.\")\n",
        "         # Fallback to original name might be safer if cleaning leads to mismatch\n",
        "         if original_name in df_cols: return original_name\n",
        "         return config_name # Return original config if neither found\n",
        "    return cleaned\n",
        "\n",
        "# Update configured names based on cleaned names IN THE DATAFRAME\n",
        "# This has all parts of the FMEA table, and uses the funciton to get cleaned names\n",
        "# ceratin cells to forward fill bc they are partialy empty in table\n",
        "COL_SUBFUNCTION = get_cleaned_name(COL_SUBFUNCTION, df.columns, column_map)\n",
        "COL_REQUIREMENTS = get_cleaned_name(COL_REQUIREMENTS, df.columns, column_map)\n",
        "COL_FAILURE_MODE = get_cleaned_name(COL_FAILURE_MODE, df.columns, column_map)\n",
        "COL_EFFECT_PRIMARY = get_cleaned_name(COL_EFFECT_PRIMARY, df.columns, column_map)\n",
        "COL_EFFECT_SECONDARY = get_cleaned_name(COL_EFFECT_SECONDARY, df.columns, column_map)\n",
        "COL_SEVERITY = get_cleaned_name(COL_SEVERITY, df.columns, column_map)\n",
        "INPUT_TEXT_COLS = [COL_SUBFUNCTION, COL_REQUIREMENTS, COL_FAILURE_MODE, COL_EFFECT_PRIMARY, COL_EFFECT_SECONDARY]\n",
        "COLS_TO_FORWARD_FILL = [COL_SUBFUNCTION, COL_REQUIREMENTS, COL_FAILURE_MODE]\n",
        "TARGET_COLUMN = COL_SEVERITY # Already potentially cleaned\n",
        "all_needed_columns = INPUT_TEXT_COLS + [TARGET_COLUMN]\n",
        "print(f\"   Using effective columns: {all_needed_columns}\")\n",
        "\n",
        "# Verify Columns Exist\n",
        "missing_cols = [col for col in all_needed_columns if col not in df.columns]; #list of columns in all_needed_columns not in df.columns, which is all columns\n",
        "if missing_cols: print(f\"❌ Error: Columns missing: {missing_cols}\"); raise ValueError(\"Missing columns\") #print out result if missing columns is not empty, and raises error\n",
        "\n",
        "# Preprocessing\n",
        "print(\"⏳ Preprocessing data...\")\n",
        "df_selected = df[all_needed_columns].copy() #only takes needed columns\n",
        "print(f\"   Forward filling columns: {COLS_TO_FORWARD_FILL}...\")\n",
        "df_selected[COLS_TO_FORWARD_FILL] = df_selected[COLS_TO_FORWARD_FILL].ffill() #forward fills the columns that need it\n",
        "initial_rows = len(df_selected); df_selected = df_selected.dropna(); final_rows = len(df_selected) #count initial rows with len(), then drops rows empty in any column, counts rows after dropping\n",
        "if initial_rows > final_rows: print(f\"   Dropped {initial_rows - final_rows} rows with NaN values.\") #prints if rows were dropped\n",
        "if final_rows == 0: raise ValueError(\"No data left after NaN drop\") #if everything was dropped, raise an Error\n",
        "\n",
        "# Convert Severity & Validate\n",
        "try:\n",
        "    df_selected[TARGET_COLUMN] = pd.to_numeric(df_selected[TARGET_COLUMN], errors='coerce') #converts values in TARGET_COLUMN into a number type, if can't turn into NaN\n",
        "    df_selected = df_selected.dropna(subset=[TARGET_COLUMN]); df_selected[TARGET_COLUMN] = df_selected[TARGET_COLUMN].astype(int) #removes rows if converted to NaN\n",
        "except Exception as e: print(f\"❌ Error converting Severity: {e}\"); raise\n",
        "#filter out data not in 1-10 range and checks if any data remains\n",
        "initial_rows = len(df_selected); df_selected = df_selected[df_selected[TARGET_COLUMN].between(1, 10)]; final_rows = len(df_selected)\n",
        "#sets df_selected to only have rows with value 1-10, then counts the new ammount of rows\n",
        "if initial_rows > final_rows: print(f\"   Removed {initial_rows - final_rows} rows with Severity outside [1, 10].\") #prints out how many rows were removed\n",
        "if final_rows == 0: raise ValueError(\"No data left with valid Severity (1-10)\")\n",
        "\n",
        "# Combine Text Features\n",
        "def combine_features(row):\n",
        "    text_parts = [] #empty list to store the text\n",
        "    for col in INPUT_TEXT_COLS: value = str(row[col]) if pd.notna(row[col]) else \"\"; clean_col_name = col.split('(')[0].strip(); text_parts.append(f\"{clean_col_name}: {value}\")\n",
        "    #goes through every input column and gets the value if it is not NaN, splits at ( and removes leading and trailing whitespace then adds it to text_parts\n",
        "    return \"\\n\".join(text_parts) #returns all text parts together with new line between them as a input for the model\n",
        "print(\"   Combining input text features into 'text' column...\")\n",
        "df_selected['text'] = df_selected.apply(combine_features, axis=1) #creates new column called text, fills it with the combined features in each row\n",
        "\n",
        "# Prepare Labels (0-9)\n",
        "df_selected['label'] = df_selected[TARGET_COLUMN] - 1 #converts from 1-10 to 0-9 because 0 is the first and populates new column label with those values\n",
        "print(f\"   Created 'label' column (0-9) from '{TARGET_COLUMN}'.\")\n",
        "\n",
        "# Keep only necessary columns\n",
        "df_final = df_selected[['text', 'label']] #just needs the combined text features and the 0-9 severity value\n",
        "\n",
        "# Create Train/Validation Split\n",
        "print(\"⏳ Splitting data...\")\n",
        "train_df, valid_df = train_test_split(df_final, test_size=0.2, random_state=42, stratify=df_final['label']) #80% for training, 20% for validation severity makes sure that severity levels are evenly distributed\n",
        "print(f\"✅ Split complete. Train size: {len(train_df)}, Validation size: {len(valid_df)}\") #prints the number of rows for training and validating\n",
        "\n",
        "# Convert to Hugging Face Datasets\n",
        "train_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n",
        "valid_dataset = Dataset.from_pandas(valid_df, preserve_index=False)\n",
        "raw_datasets = DatasetDict({'train': train_dataset, 'validation': valid_dataset}) #create DatasetDict with train and validation and corresponding datasets which is needed for Trainer\n",
        "print(\"✅ Data prepared and converted to Hugging Face Datasets format.\")\n",
        "print(raw_datasets)\n",
        "\n",
        "#  cleanup\n",
        "import gc; del df, df_selected, df_final, train_df, valid_df; gc.collect() #imports garbage collector, deletes unneeded large pandas dataframes for memory saving, and runs garbage collector for memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fc-fpWD0-0va"
      },
      "outputs": [],
      "source": [
        "# --- Hugging Face Login---\n",
        "print(\"\\nPlease log in to Hugging Face using an Access Token with 'read' permission.\")\n",
        "notebook_login()\n",
        "print(\"✅ Login process initiated.\")\n",
        "# --- End Login ---\n",
        "print(f\"\\n⏳ Loading tokenizer for '{MODEL_NAME}'...\")\n",
        "\n",
        "# Load tokenizer associated with the base model\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME) # Login above handles token\n",
        "    # Set padding token (Llama 3 uses EOS)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        print(f\"   Tokenizer pad_token set to eos_token: {tokenizer.pad_token}\")\n",
        "    print(\"✅ Tokenizer loaded.\")\n",
        "except Exception as e: print(f\"❌ Error loading tokenizer: {e}\"); raise\n",
        "\n",
        "# Load tokenizer associated with the base model\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME) # Login above handles token\n",
        "    # Set padding token (Llama 3 uses EOS)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token #is padding token isnt defined sets the pad token to be same as eos token\n",
        "        print(f\"   Tokenizer pad_token set to eos_token: {tokenizer.pad_token}\")\n",
        "    print(\"✅ Tokenizer loaded.\")\n",
        "except Exception as e: print(f\"❌ Error loading tokenizer: {e}\"); raise\n",
        "\n",
        "# Define tokenization function\n",
        "def tokenize_function(examples):\n",
        "    #this tokenizes the \"text\" column and truncation=true tells it to cut off the at the assigned legnth\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=MAX_SEQ_LENGTH, padding=False)\n",
        "\n",
        "print(\"⏳ Tokenizing datasets...\")\n",
        "#tokenizes entire training and validation data sets\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "#assigns the result of map() to tokenized datasets\n",
        "#map applies the tokenize funciton in batches\n",
        "#removes \"text\" column from tokenized datasets\n",
        "print(\"✅ Datasets tokenized.\")\n",
        "print(tokenized_datasets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NB4THQKd6h8m"
      },
      "outputs": [],
      "source": [
        "# Load Llama Model\n",
        "\n",
        "\n",
        "print(\"⚙️ Defining 4-bit quantization config (BitsAndBytesConfig)...\")\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, #important setting that enables quantization\n",
        "    bnb_4bit_quant_type=\"nf4\", #type of quanitzation to use\n",
        "    bnb_4bit_compute_dtype=compute_dtype, # Determined in Cell 3 based on GPU\n",
        "    bnb_4bit_use_double_quant=True, #quantizes already quantized data for memory savings\n",
        ")\n",
        "print(\"✅ Quantization config defined.\")\n",
        "\n",
        "# --- Load Base Model with Quantization ---\n",
        "print(f\"⏳ Loading base model '{MODEL_NAME}' for Sequence Classification with 4-bit quantization...\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME, #name of the model\n",
        "    quantization_config=bnb_config, #passed config from earlier to tell the model to load with 4-bit quantization\n",
        "    device_map=\"auto\", # Use \"auto\" for standard HF, should work better on A100\n",
        "    # device_map = {\"\": 0}, # Use explicit mapping if \"auto\" causes issues\n",
        "    num_labels=NUM_LABELS, #number of output lables, 10 for this case\n",
        "    id2label=id2label, #dictionary mapping 0-9 to \"1\"-\"10\"\n",
        "    label2id=label2id, #dicionary map the other way\n",
        "    # ignore_mismatched_sizes=True # Try uncommenting if size mismatch error occurs\n",
        ")\n",
        "print(\"✅ Base model loaded with quantization.\")\n",
        "\n",
        "# Set pad token ID in model config if tokenizer has one (important!)\n",
        "if tokenizer.pad_token_id is not None: #checks for a padding token\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id #if there is a padding token, this sets it in the config\n",
        "    print(f\"Model pad_token_id set to: {model.config.pad_token_id}\")\n",
        "\n",
        "# --- Prepare Model for K-bit Training & Apply LoRA using PEFT ---\n",
        "print(\"⚙️ Preparing model for K-bit training and defining LoRA config (PEFT)...\")\n",
        "model.gradient_checkpointing_enable() # Often needed for K-bit training, trades memory for computation\n",
        "model = prepare_model_for_kbit_training(model) #further prepares model\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_R, #sets lora rank to previously defined\n",
        "    lora_alpha=LORA_ALPHA, #scaling factor, previously defined\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", # Standard Llama 3 targets\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=LORA_DROPOUT, #sets dropout probability to previously defined\n",
        "    bias=\"none\", #number of bias terms in LoRA matricies\n",
        "    task_type=TaskType.SEQ_CLS, # Specify Sequence Classification task\n",
        ")\n",
        "print(\"✅ LoRA configuration defined.\")\n",
        "\n",
        "print(\"⚡️ Applying LoRA adapter to the model using PEFT...\")\n",
        "model = get_peft_model(model, lora_config) # Standard PEFT function, returns new model with LoRA adapters\n",
        "print(\"✅ LoRA adapter applied.\")\n",
        "model.print_trainable_parameters() #prints summary of the model's paramaters\n",
        "\n",
        "# --- Data Collator ---\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer) #pads things to maximum legnth\n",
        "print(\"✅ Data collator created.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5zsY0wquZtXX"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from transformers import TrainingArguments, Trainer # Ensure these are imported\n",
        "import torch # Ensure torch is imported\n",
        "\n",
        "# --- Define Compute Metrics Function ---\n",
        "# Keep this function as it's needed for manual evaluation later\n",
        "def compute_metrics(eval_pred): #function that takes in a tuple with arguments of the prediction and true label\n",
        "    predictions, labels = eval_pred; preds = np.argmax(predictions, axis=1) #unpacks tuple, and calculates predicted class index\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=0) #computers precision, recall, and f1 scores\n",
        "    acc = accuracy_score(labels, preds); return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}\n",
        "    #compares accuracy by comparing true labels and prediction\n",
        "\n",
        "# --- Define Training Arguments (Workaround Applied) ---\n",
        "print(\"⚙️ Setting Training Arguments (evaluation_strategy workaround)...\")\n",
        "\n",
        "# Check GPU capability for fp16/bf16 (should be done in Cell 3, but check again is ok)\n",
        "bf16_supported = False\n",
        "fp16_enabled = False\n",
        "if torch.cuda.is_available():\n",
        "    if torch.cuda.get_device_capability()[0] >= 8: # Ampere+ (A100, etc.)\n",
        "        bf16_supported = True\n",
        "        print(\"   Setting bf16=True for Ampere+ GPU.\")\n",
        "    else: # T4, V100, etc.\n",
        "        fp16_enabled = True\n",
        "        print(\"   Setting fp16=True for non-Ampere GPU.\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR + \"_chkpts\", # Use OUTPUT_DIR from Cell 3 config\n",
        "    # --- Training Duration & Batching (Use config from Cell 3) ---\n",
        "    num_train_epochs = NUM_EPOCHS,\n",
        "    # max_steps = MAX_STEPS, # Alternatively use max_steps\n",
        "    per_device_train_batch_size=BATCH_SIZE_PER_DEVICE, #from cell 3\n",
        "    gradient_accumulation_steps=GRAD_ACCUMULATION_STEPS, #from cell 3\n",
        "    learning_rate=LEARNING_RATE, #from cell 3\n",
        "\n",
        "    # --- Optimizer & Precision ---\n",
        "    optim=\"paged_adamw_8bit\", # Recommended 8-bit optimizer for QLoRA\n",
        "    fp16=fp16_enabled,        # Enable based on GPU check\n",
        "    bf16=bf16_supported,      # Enable based on GPU check\n",
        "\n",
        "    # --- Logging & Saving ---\n",
        "    logging_strategy=\"steps\", #determines when the log info\n",
        "    logging_steps=LOGGING_STEPS, #from cell 3\n",
        "    save_strategy=SAVE_STRATEGY,     # e.g., \"epoch\" or \"steps\"\n",
        "    # save_steps = SAVE_STEPS,      # Use if save_strategy=\"steps\"\n",
        "    save_total_limit=1,          # Optional: keep only last/best checkpoint\n",
        "\n",
        "    # --- WORKAROUND APPLIED ---\n",
        "    # evaluation_strategy=\"epoch\", # <<< COMMENTED OUT / REMOVED\n",
        "    # load_best_model_at_end=True, # <<< MUST be False if not evaluating during training\n",
        "    # metric_for_best_model=\"f1\",  # <<< Comment out / remove\n",
        "    load_best_model_at_end=False,  # Explicitly set to False\n",
        "\n",
        "    # --- Other Args ---\n",
        "    seed=42, #sets the seed for repoducibility\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=True, # Safe if Cell 5 removed 'text' column\n",
        "    gradient_checkpointing=True, # Recommended for standard QLoRA memory saving\n",
        "    gradient_checkpointing_kwargs={'use_reentrant':False},\n",
        ")\n",
        "\n",
        "# --- Create Trainer ---\n",
        "print(\"⚙️ Creating Trainer...\")\n",
        "# Ensure model, tokenized_datasets, tokenizer, data_collator exist from previous cells\n",
        "try:\n",
        "    #creates trainer with all of the configs from earlier\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_datasets[\"train\"],\n",
        "        eval_dataset=tokenized_datasets[\"validation\"], # Keep for manual eval\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics, # Keep for manual eval\n",
        "    )\n",
        "    print(\"✅ Trainer created.\")\n",
        "except NameError as ne:\n",
        "    print(f\"❌ NameError: A required object (model, dataset, etc.) not found: {ne}\")\n",
        "    print(\"   Please ensure Cells 3, 4, 5, 6 ran successfully.\")\n",
        "    raise\n",
        "except Exception as e:\n",
        "    print(f\"❌ Unexpected error creating Trainer: {e}\")\n",
        "    raise\n",
        "\n",
        "# --- Start Training ---\n",
        "print(f\"\\n🚀🚀🚀 Starting Standard QLoRA Fine-tuning! 🚀🚀🚀\")\n",
        "try:\n",
        "    train_result = trainer.train() # Train the model\n",
        "    print(\"\\n✅✅✅ Training finished! ✅✅✅\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ An error occurred during trainer.train(): {e}\")\n",
        "    raise\n",
        "\n",
        "# --- !! Manually Evaluate Model AFTER Training !! ---\n",
        "print(\"\\n🧪 Evaluating model after training has completed...\")\n",
        "try:\n",
        "    eval_results = trainer.evaluate(eval_dataset=tokenized_datasets[\"validation\"]) #evaluates model based on validation dataset\n",
        "    print(\"\\n📊 Final Validation Set Evaluation Results (Manual Trigger):\")\n",
        "    print(eval_results)\n",
        "    trainer.log_metrics(\"eval_manual\", eval_results)\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error during manual evaluation: {e}\")\n",
        "\n",
        "# --- Save Final Model State ---\n",
        "# Note: Saves the model state at the END of training.\n",
        "print(f\"\\n💾 Saving final trained model adapter & tokenizer to '{OUTPUT_DIR}'...\")\n",
        "try:\n",
        "    trainer.save_model(OUTPUT_DIR)\n",
        "    if 'tokenizer' in locals() and tokenizer is not None: # Save tokenizer if available\n",
        "         tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "    print(f\"✅ Final model adapter and tokenizer saved to '{OUTPUT_DIR}'.\")\n",
        "except Exception as e:\n",
        "     print(f\"❌ Error saving model/tokenizer: {e}\")\n",
        "\n",
        "# --- Optional: Clean up GPU memory ---\n",
        "import gc\n",
        "# Add del statements for objects no longer needed\n",
        "# Example: del model, trainer, tokenized_datasets, raw_datasets\n",
        "gc.collect()\n",
        "if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "print(\"\\n🧹 Training cell GPU memory cache potentially cleared.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZG5H8td4c7Xi"
      },
      "outputs": [],
      "source": [
        "# --- Optional Download Code Block (Run in new cell after Cell 7) ---\n",
        "import shutil\n",
        "from google.colab import files\n",
        "import os\n",
        "import time\n",
        "\n",
        "folder_to_download = \"fmea_severity_classifier_llama31_8b_standard_qlora\" # used later\n",
        "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "zip_filename = f\"{folder_to_download}_{timestamp}.zip\"\n",
        "\n",
        "print(f\"\\n📦 Preparing folder '{folder_to_download}' for download...\")\n",
        "try:\n",
        "    if os.path.exists(folder_to_download):\n",
        "        print(f\"   Zipping folder to '{zip_filename}'...\")\n",
        "        shutil.make_archive(folder_to_download, 'zip', folder_to_download)\n",
        "        print(f\"   Zipping complete.\")\n",
        "        print(f\"⬇️ Triggering browser download for '{zip_filename}'...\")\n",
        "        files.download(zip_filename) # Trigger download\n",
        "        print(f\"✅ Download initiated. Check your browser.\")\n",
        "    else:\n",
        "        print(f\"❌ Error: Output directory '{folder_to_download}' not found. Cannot download.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ An error occurred during zipping or downloading: {e}\")\n",
        "# --- End Download Code Block ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hvZgIcYOdTJh"
      },
      "outputs": [],
      "source": [
        "# Evaluate on Validation Set\n",
        "# Reloads model if needed and prints report\n",
        "\n",
        "\n",
        "try:\n",
        "    trainer # Check if trainer from Cell 7 exists\n",
        "    trainer_to_use = trainer\n",
        "    # Ensure dataset and mappings are accessible\n",
        "    dataset_to_eval = tokenized_datasets[\"validation\"]\n",
        "    id2label_eval = id2label\n",
        "    NUM_LABELS_EVAL = NUM_LABELS\n",
        "    print(\"Using existing trainer object for prediction.\")\n",
        "except NameError:\n",
        "    print(\"Trainer object not found. Loading model from disk (Standard QLoRA)...\")\n",
        "    from transformers import AutoTokenizer, AutoModelForSequenceClassification, BitsAndBytesConfig, Trainer, TrainingArguments\n",
        "    from peft import PeftModel\n",
        "    import torch\n",
        "\n",
        "    ADAPTER_PATH_EVAL = OUTPUT_DIR # Use OUTPUT_DIR from Cell 3\n",
        "    MODEL_NAME_EVAL = MODEL_NAME # Use MODEL_NAME from Cell 3\n",
        "    # Reload tokenizer\n",
        "    tokenizer_eval = AutoTokenizer.from_pretrained(ADAPTER_PATH_EVAL) #loads from saved files\n",
        "    if tokenizer_eval.pad_token is None: tokenizer_eval.pad_token = tokenizer_eval.eos_token #same setup as training\n",
        "    # Reload base model with quantization config\n",
        "    compute_dtype_eval = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else torch.float16\n",
        "    bnb_config_eval = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=compute_dtype_eval, bnb_4bit_use_double_quant=True)\n",
        "    base_model_eval = AutoModelForSequenceClassification.from_pretrained(\n",
        "        MODEL_NAME_EVAL, quantization_config=bnb_config_eval, device_map=\"auto\",\n",
        "        num_labels=NUM_LABELS, id2label=id2label, label2id=label2id\n",
        "    )\n",
        "    if base_model_eval.config.pad_token_id is None: base_model_eval.config.pad_token_id = tokenizer_eval.pad_token_id\n",
        "    #sets padding token id if not already set\n",
        "\n",
        "    # Load adapter\n",
        "    model_eval = PeftModel.from_pretrained(base_model_eval, ADAPTER_PATH_EVAL)\n",
        "    model_eval.eval()\n",
        "    print(\"Model reloaded from disk.\")\n",
        "    # Create dummy trainer for .predict()\n",
        "    dummy_args = TrainingArguments(output_dir=\"./eval_temp_std\", report_to=\"none\", device=model_eval.device)\n",
        "    eval_trainer = Trainer(model=model_eval, args=dummy_args, tokenizer=tokenizer_eval)\n",
        "    trainer_to_use = eval_trainer\n",
        "    # Need to re-run tokenization if 'tokenized_datasets' not available\n",
        "    # Assuming it's available or re-run Cell 5\n",
        "    dataset_to_eval = tokenized_datasets[\"validation\"]\n",
        "    id2label_eval = id2label; NUM_LABELS_EVAL = NUM_LABELS\n",
        "\n",
        "# Get predictions\n",
        "predictions_output = trainer_to_use.predict(dataset_to_eval) #pass in eval dataset and runs the model\n",
        "y_true = predictions_output.label_ids #holds the true labels\n",
        "y_pred = np.argmax(predictions_output.predictions, axis=1) #calculates predicted label based on raw outputs\n",
        "\n",
        "# Generate report using label names (\"1\" to \"10\")\n",
        "# Define the full range of expected label indices (0 to 9)\n",
        "expected_labels = list(range(NUM_LABELS_EVAL)) # Should be [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "# Ensure target_names correspond to these expected labels\n",
        "target_names = [id2label_eval[i] for i in expected_labels]\n",
        "\n",
        "# Call classification_report with the 'labels' parameter specified\n",
        "report = classification_report(\n",
        "    y_true,\n",
        "    y_pred,\n",
        "    labels=expected_labels, # <<< Tell function to report these labels\n",
        "    target_names=target_names,\n",
        "    digits=4,\n",
        "    zero_division=0\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qepGIxsVi5CZ"
      },
      "outputs": [],
      "source": [
        "# ───────────────────────────────────────────────────────────\n",
        "# Cell 8A: Run Prediction & Inspect Results\n",
        "# ───────────────────────────────────────────────────────────\n",
        "import numpy as np\n",
        "import pandas as pd # Needed for unique check potentially\n",
        "# Make sure necessary libraries/objects from previous cells are loaded\n",
        "# (Trainer, tokenized_datasets, id2label, NUM_LABELS, etc.)\n",
        "print(\"\\n📋 Preparing for detailed classification report...\")\n",
        "# --- Logic to find or reload trainer and data ---\n",
        "try:\n",
        "    trainer # Check if trainer from Cell 7 exists\n",
        "    # Ensure needed variables are accessible\n",
        "    if 'trainer_to_use' not in locals(): trainer_to_use = trainer\n",
        "    if 'dataset_to_eval' not in locals(): dataset_to_eval = tokenized_datasets[\"validation\"]\n",
        "    if 'id2label_eval' not in locals(): id2label_eval = id2label\n",
        "    if 'NUM_LABELS_EVAL' not in locals(): NUM_LABELS_EVAL = NUM_LABELS\n",
        "    print(\"Using existing trainer object and data for prediction.\")\n",
        "except NameError:\n",
        "    print(\"Trainer object or other necessary variables not found. Attempting to reload model...\")\n",
        "    # Include the reloading logic from your original Cell 8 here if needed\n",
        "    # Make sure ADAPTER_PATH_EVAL, MODEL_NAME_EVAL etc. are defined correctly based on Cell 3/7\n",
        "    # For simplicity, assuming Cell 7 objects still exist. Add reloading if required.\n",
        "    print(\"Error: Cannot proceed without trainer object or reloaded model. Please ensure Cell 7 ran or add reloading code.\")\n",
        "    raise NameError(\"Trainer not found and reloading logic missing/failed.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error setting up for prediction: {e}\")\n",
        "    raise\n",
        "# --- End finding trainer/data ---\n",
        "\n",
        "# --- Get Predictions ---\n",
        "print(f\"\\n⏳ Running prediction on validation set ({len(dataset_to_eval)} samples)...\")\n",
        "try:\n",
        "    predictions_output = trainer_to_use.predict(dataset_to_eval) #passes the model in, and runs forward pass of model without back prop\n",
        "    print(\"✅ trainer.predict() finished successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error during trainer.predict(): {e}\")\n",
        "    raise # Stop if prediction fails\n",
        "\n",
        "# --- Inspect Prediction Outputs ---\n",
        "try:\n",
        "    y_true = predictions_output.label_ids #true labels\n",
        "    y_pred = np.argmax(predictions_output.predictions, axis=1) #predicted labels by taking raw outputs\n",
        "    print(\"\\n--- Prediction Output Inspection ---\")\n",
        "    print(f\"Shape of y_true (true labels): {y_true.shape}\")\n",
        "    print(f\"Shape of y_pred (predicted labels): {y_pred.shape}\")\n",
        "    print(f\"Unique true labels found in validation set: {np.unique(y_true)}\")\n",
        "    print(f\"Unique predicted labels by the model: {np.unique(y_pred)}\")\n",
        "    print(f\"Data type of y_true: {y_true.dtype}\")\n",
        "    print(f\"Data type of y_pred: {y_pred.dtype}\")\n",
        "    print(f\"Any NaN in y_true?: {np.isnan(y_true).any()}\")\n",
        "    # y_pred from argmax should not contain NaN unless logits were NaN\n",
        "    print(\"------------------------------------\")\n",
        "    print(\"\\n✅ Inspection complete. If shapes look correct and labels are in range [0-9], proceed to Cell 8B.\")\n",
        "\n",
        "    # Make variables available for the next cell (Colab usually does this automatically)\n",
        "    # If issues arise, you might need to declare them global, but try without first.\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error during result inspection: {e}\")\n",
        "    raise\n",
        "# --- End Inspection ---\n",
        "\n",
        "# NOTE: We stop here and run the report generation in the next cell (Cell 8B)\n",
        "# %%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iW6-9qI3kNA4"
      },
      "outputs": [],
      "source": [
        "#Generate and Print Report\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "# Make sure necessary variables exist from previous cell's execution\n",
        "# (y_true, y_pred, id2label_eval, NUM_LABELS_EVAL)\n",
        "\n",
        "print(\"\\n⚙️ Preparing to generate classification report...\")\n",
        "\n",
        "try:\n",
        "    # Check if needed variables exist\n",
        "    y_true\n",
        "    y_pred\n",
        "    id2label_eval\n",
        "    NUM_LABELS_EVAL\n",
        "\n",
        "    # Define the full range of expected label indices (0 to 9)\n",
        "    expected_labels = list(range(NUM_LABELS_EVAL))\n",
        "    # Ensure target_names correspond to these expected labels\n",
        "    target_names = [id2label_eval[i] for i in expected_labels]\n",
        "\n",
        "    print(\"⏳ Calculating classification report...\")\n",
        "    # Call classification_report with the 'labels' parameter specified\n",
        "    report = classification_report(\n",
        "        y_true,\n",
        "        y_pred,\n",
        "        labels=expected_labels, # Tell function all expected labels\n",
        "        target_names=target_names,\n",
        "        digits=4,\n",
        "        zero_division=0 # Handle labels with no predictions/support\n",
        "    )\n",
        "\n",
        "    print(\"\\n✅ Report calculation finished.\")\n",
        "    print(\"\\n📋 Classification Report:\\n\")\n",
        "    print(report) # <<< Print the calculated report\n",
        "\n",
        "except NameError as ne:\n",
        "     print(f\"❌ NameError: A required variable (y_true, y_pred, etc.) is missing: {ne}\")\n",
        "     print(\"   Please ensure Cell 8A ran successfully first.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error during classification_report generation or printing: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc() # Print detailed traceback for errors here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "i7U625uxkoKp"
      },
      "outputs": [],
      "source": [
        "# ───────────────────────────────────────────────────────────\n",
        "# Cell 9: Manual Prediction Function (Standard QLoRA for Llama 3.1)\n",
        "# ───────────────────────────────────────────────────────────\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# import re # Not needed for classification output\n",
        "\n",
        "# --- Configuration ---\n",
        "MODEL_NAME = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "ADAPTER_PATH = \"fmea_severity_classifier_llama31_8b_standard_qlora\" # <<< same path at cell 7\n",
        "MAX_SEQ_LENGTH = 512\n",
        "# Define input columns EXACTLY as used in training (Cell 4)\n",
        "# Assumes these were correctly defined/cleaned before\n",
        "COL_SUBFUNCTION = \"Subfunction\"; COL_REQUIREMENTS = \"Requirements\"; COL_FAILURE_MODE = \"Potential Failure Mode and descriptions\"\n",
        "COL_EFFECT_PRIMARY = \"Potential Effect(s) of Failure (primary)\"; #COL_EFFECT_SECONDARY = \"Potential Effect(s) of Failure (secondary)\" #cant use right now\n",
        "INPUT_COLS_MANUAL = [COL_SUBFUNCTION, COL_REQUIREMENTS, COL_FAILURE_MODE, COL_EFFECT_PRIMARY] #, COL_EFFECT_SECONDARY]\n",
        "NUM_LABELS = 10\n",
        "id2label = {i: str(i+1) for i in range(NUM_LABELS)}\n",
        "# --- End Configuration ---\n",
        "\n",
        "# --- Load Fine-tuned Model and Tokenizer ---\n",
        "# Ensure this loading logic runs successfully before prediction\n",
        "print(\"⏳ Loading fine-tuned Llama 3.2 model for manual prediction (Standard QLoRA)...\")\n",
        "try:\n",
        "    # Define quantization config again\n",
        "    compute_dtype_pred = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else torch.float16\n",
        "    bnb_config_pred = BitsAndBytesConfig(\n",
        "        load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=compute_dtype_pred, bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "    # Load base model with quantization, similar as before\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        quantization_config=bnb_config_pred,\n",
        "        device_map=\"auto\", # Or {\"\": 0}\n",
        "        num_labels=NUM_LABELS,\n",
        "        id2label=id2label,\n",
        "        label2id={v: k for k, v in id2label.items()},\n",
        "        # token = \"hf_...\" # Add if login via notebook_login() didn't persist\n",
        "    )\n",
        "    # Load the tokenizer associated with the saved adapter/base\n",
        "    tokenizer = AutoTokenizer.from_pretrained(ADAPTER_PATH) # Load from adapter path\n",
        "    if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
        "    if model.config.pad_token_id is None: model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    # Load the LoRA adapter onto the base model\n",
        "    print(f\"   Applying LoRA adapter from {ADAPTER_PATH}...\")\n",
        "    model = PeftModel.from_pretrained(model, ADAPTER_PATH)\n",
        "    model.eval()\n",
        "    device = model.device\n",
        "    print(f\"✅ Model and tokenizer loaded on device: {device}\")\n",
        "\n",
        "except Exception as e: print(f\"❌ Error loading model/adapter: {e}\"); raise\n",
        "# --- End Model Loading ---\n",
        "\n",
        "# --- Define Prediction Function ---\n",
        "def predict_fmea_severity_final(**kwargs):\n",
        "    \"\"\" Takes keyword arguments for FMEA input features and predicts Severity (1-10). \"\"\"\n",
        "    # Build the input text string\n",
        "    text_parts = []; missing_args = []\n",
        "    for col in INPUT_COLS_MANUAL:\n",
        "        value = kwargs.get(col); value = str(value) if pd.notna(value) else \"\"\n",
        "        clean_col_name = col.split('(')[0].strip(); text_parts.append(f\"{clean_col_name}: {value}\")\n",
        "    combined_text = \"\\n\".join(text_parts)\n",
        "    print(f\"--- Input Text for Model ---\\n{combined_text}\\n--------------------------\")\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer([combined_text], return_tensors=\"pt\", truncation=True, padding=True, max_length=MAX_SEQ_LENGTH).to(device)\n",
        "\n",
        "    # Predict\n",
        "    print(\"⏳ Predicting severity...\")\n",
        "    with torch.no_grad(): outputs = model(**inputs); logits = outputs.logits\n",
        "    predicted_class_id = torch.argmax(logits, dim=-1).item()\n",
        "    predicted_severity = id2label.get(predicted_class_id, \"Unknown\") # Use mapping\n",
        "\n",
        "    print(f\"✅ Predicted Severity (1-10): {predicted_severity}\")\n",
        "    return predicted_severity\n",
        "\n",
        "# --- Example Usage (Using User Provided Scenarios)\n",
        "print(\"\\n--- Manual Prediction Examples (User Provided) ---\")\n",
        "\n",
        "# Example 1: Emergency Maneuvers\n",
        "print(\"--- Predicting User Example 1 ---\")\n",
        "pred_user_1 = predict_fmea_severity_final(\n",
        "    # Use **{} for keys with spaces/symbols, ensure keys match cleaned column names\n",
        "    **{COL_SUBFUNCTION: \"Emergency Maneuvers\",\n",
        "       COL_REQUIREMENTS: \"Manage safe operations by reacting to sudden braking or lane changes by other vehicles or objects\",\n",
        "       COL_FAILURE_MODE: \"No Function (The autonomous truck fails to detect or react appropriately [brake, steer] to sudden braking, lane changes by other vehicles, or objects appearing in the path, thereby failing to manage safe operations during emergency scenarios.)\",\n",
        "       COL_EFFECT_PRIMARY: \"AV fails to apply required emergency braking\",\n",
        "       COL_EFFECT_SECONDARY: \"results in traffic citation\"}\n",
        ")\n",
        "print(f\"Predicted Severity for User Example 1: {pred_user_1}\\n\")\n",
        "\n",
        "# Example 2: Move For Disabled/Stopped Vehicles\n",
        "print(\"--- Predicting User Example 2 ---\")\n",
        "pred_user_2 = predict_fmea_severity_final(\n",
        "    **{COL_SUBFUNCTION: \"Move For Disabled/Stopped Vehicles\",\n",
        "       COL_REQUIREMENTS: \"Manage safe operations by operating appropriately to disabled or emergency vehicles that are stationary or stopped on the road or on the shoulder.\",\n",
        "       COL_FAILURE_MODE: \"No Function (The autonomous truck fails to detect a stationary disabled/emergency vehicle or fails to execute required safe operations like reducing speed, changing lanes [moving over], or providing adequate lateral clearance, thereby failing to manage safe operations.)\",\n",
        "       COL_EFFECT_PRIMARY: \"AV fails to reduce speed when approaching stationary vehicle/personnel\",\n",
        "       COL_EFFECT_SECONDARY: \"results in traffic citation\"}\n",
        ")\n",
        "print(f\"Predicted Severity for User Example 2: {pred_user_2}\\n\")\n",
        "\n",
        "# --- End Example Usage ---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMW+OIQRr8QX4xxtn2Z4nac",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}