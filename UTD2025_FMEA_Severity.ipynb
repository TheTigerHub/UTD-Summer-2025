{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMMC4BBw8AEsjqMYlYDq7IN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5c4853a05da041b0a0bc1a0a5063d383": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5200a4039bea4651acd504a7fc4f116c"
            ],
            "layout": "IPY_MODEL_d188c2ef85fa4e5fb60aecfb9c4a44d7"
          }
        },
        "e8738f16d2dc445789e23c260fe39d03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1a0d955d57f4004b0f12223d63aa335",
            "placeholder": "​",
            "style": "IPY_MODEL_482c46f10dc54644bab78ce150d9e8ae",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "b836fcd876cc43008532a4698ef340ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_32693348a7414f67aa63e9590cffe719",
            "placeholder": "​",
            "style": "IPY_MODEL_f21bb392714547c4805a9f9f5170df1d",
            "value": ""
          }
        },
        "1a8be729065c48968b2bfef5de3cc541": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_6ec77183777f423d928fb9ab37c85c80",
            "style": "IPY_MODEL_05bcc2d2b1b3473cba1ce56a83203c57",
            "value": false
          }
        },
        "8cbb9ba3c4c843fe921bdc764d7744d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_79fb42f9e04b4c5981594a848aa87dcc",
            "style": "IPY_MODEL_14ae1aa481ff4640bf6d84394596043f",
            "tooltip": ""
          }
        },
        "0079dd9f60c641de99c36a2a31ea8e29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c9fcb6fc5e74859836f1820ab14e7d3",
            "placeholder": "​",
            "style": "IPY_MODEL_d4de73971cc342be82926b7f6dc8ce4c",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "d188c2ef85fa4e5fb60aecfb9c4a44d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "e1a0d955d57f4004b0f12223d63aa335": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "482c46f10dc54644bab78ce150d9e8ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32693348a7414f67aa63e9590cffe719": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f21bb392714547c4805a9f9f5170df1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ec77183777f423d928fb9ab37c85c80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05bcc2d2b1b3473cba1ce56a83203c57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "79fb42f9e04b4c5981594a848aa87dcc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14ae1aa481ff4640bf6d84394596043f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "0c9fcb6fc5e74859836f1820ab14e7d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4de73971cc342be82926b7f6dc8ce4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8654a33c5ae547f2a04c6051baab3dcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d74729e7a4954e0389a0b331deeefb21",
            "placeholder": "​",
            "style": "IPY_MODEL_258c5e137e114a1fa4c080f4865a2449",
            "value": "Connecting..."
          }
        },
        "d74729e7a4954e0389a0b331deeefb21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "258c5e137e114a1fa4c080f4865a2449": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5200a4039bea4651acd504a7fc4f116c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19b0fade90c34f6eb5fbcc52d02f1902",
            "placeholder": "​",
            "style": "IPY_MODEL_99080334ed4e43948133ce21a0c4aabe",
            "value": "Invalid user token."
          }
        },
        "19b0fade90c34f6eb5fbcc52d02f1902": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99080334ed4e43948133ce21a0c4aabe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7cf3d13d08ad46268f23ab5749016790": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d35b0361011447cab5594587ac72a81a",
              "IPY_MODEL_5abe9f12e71c45eab0b7058dfa41ac54",
              "IPY_MODEL_b238dea034a8403aa7f063c523e9d2a6"
            ],
            "layout": "IPY_MODEL_b900e00fce744e64b9227bb63e73a3da"
          }
        },
        "d35b0361011447cab5594587ac72a81a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_895c7bc8f9c04aaab5890331654a1414",
            "placeholder": "​",
            "style": "IPY_MODEL_c0ae4793a3764fd0a82a215b4db0266b",
            "value": "model.safetensors: 100%"
          }
        },
        "5abe9f12e71c45eab0b7058dfa41ac54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03f839a808334cee97e63fda2ea710b6",
            "max": 2471645608,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_94ce2b6d236a431494ac1e068d0db533",
            "value": 2471645608
          }
        },
        "b238dea034a8403aa7f063c523e9d2a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1d111be47d1451ea38465e2d7a532a1",
            "placeholder": "​",
            "style": "IPY_MODEL_7f3f79e352d84f90b9e877cce1091d5d",
            "value": " 2.47G/2.47G [01:08&lt;00:00, 21.4MB/s]"
          }
        },
        "b900e00fce744e64b9227bb63e73a3da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "895c7bc8f9c04aaab5890331654a1414": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0ae4793a3764fd0a82a215b4db0266b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "03f839a808334cee97e63fda2ea710b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94ce2b6d236a431494ac1e068d0db533": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f1d111be47d1451ea38465e2d7a532a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f3f79e352d84f90b9e877cce1091d5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheTigerHub/UTD-Summer-2025/blob/main/UTD2025_FMEA_Severity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgwOMXjswnBk",
        "outputId": "e9b0117c-c7a8-47b5-9348-7f5ecc4347de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "installing hugging face libraries for QLoRA\n",
            "installed accelerate\n",
            "installed peft\n",
            "installed bitsandbytes\n",
            "installed transformers\n",
            "installed datasets\n",
            "installed scikit-learn\n",
            "installed pandas\n",
            "installed openpyxl\n",
            "\n",
            " libraries installed\n"
          ]
        }
      ],
      "source": [
        "print (\"installing hugging face libraries for QLoRA\")\n",
        "\n",
        "!pip install -q accelerate==0.30.1 #hardware acceleration\n",
        "print (\"installed accelerate\")\n",
        "!pip install -q peft==0.11.1 #peft is parameter effecient fine tuning which allows LoRA and QLoRA\n",
        "print (\"installed peft\")\n",
        "!pip install -q bitsandbytes #bits and bytes helps with quantization\n",
        "print(\"installed bitsandbytes\")\n",
        "!pip install -q transformers==4.41.1 #hugging face library for pre-trained models, tokenizers\n",
        "print(\"installed transformers\")\n",
        "!pip install -q datasets==2.19.0 #datasets is used for loading, processing, and managing datasets\n",
        "print(\"installed datasets\")\n",
        "!pip install -q scikit-learn==1.4.2 # scikit-learn for data splitting\n",
        "print(\"installed scikit-learn\")\n",
        "!pip install -q pandas #used for data manipulation and analysis\n",
        "print(\"installed pandas\")\n",
        "!pip install -q openpyxl #read excel\n",
        "print(\"installed openpyxl\")\n",
        "\n",
        "print (\"\\n libraries installed\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"installing hugging face libraries for QLoRA\")\n",
        "\n",
        "!pip install -q \"peft[bnb]\" --upgrade #installs peft pacakge with bnb extra, integrates peft and bnb\n",
        "\n",
        "!pip install -q accelerate bitsandbytes \"transformers[torch]\" --upgrade #installs transformers with dependencies for PyTorch\n",
        "\n",
        "!pip install -q datasets==2.19.0 scikit-learn==1.4.2 pandas openpyxl #installs datasets, scikit-learn, pandas, and openpyxl\n",
        "\n",
        "print (\"\\n libraries installed\")\n",
        "print (\"\\n restart runtime\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EI5WZYrHKSOI",
        "outputId": "5ad11faf-691e-47f1-b134-73ef27c27ee0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "installing hugging face libraries for QLoRA\n",
            "\u001b[33mWARNING: peft 0.15.2 does not provide the extra 'bnb'\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            " libraries installed\n",
            "\n",
            " restart runtime\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#imports libraries and configures data and training\n",
        "\n",
        "import torch #torch is necessary for the Llama model\n",
        "import pandas as pd #will be used later to load, clean, and process data before converting to a hugginf face dataset\n",
        "import numpy as np #library for large, multi dimensional arrays and matricies as well as operations on them\n",
        "from datasets import Dataset, DatasetDict #provides effecient way to handle large datasets for machine learning, espicially natural language processing\n",
        "from sklearn.model_selection import train_test_split #from sci-kit learn library, splits dataset into training and validation\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report #these provide information on the accuracy of the model\n",
        "from transformers import ( #key components from transformers\n",
        "    AutoTokenizer, #loads correct tokenize automatically, tokenizers convert text into numerical IDs\n",
        "    AutoModelForSequenceClassification, #standard class the automatically load the correct model for sequence classification tasks\n",
        "    TrainingArguments, #class to configure training\n",
        "    Trainer, #class that simplifies training and eval loop for hugging face models\n",
        "    DataCollatorWithPadding, #pads sequences of data in a batch to be the same legnth\n",
        "    BitsAndBytesConfig # manual QLoRA config, quantization is important bc of GPU resources\n",
        ")\n",
        "from peft import (\n",
        "    get_peft_model, #wraps model with peft adapter like QLoRA\n",
        "    LoraConfig, #class for LoRA config\n",
        "    TaskType, #specifies task type\n",
        "    prepare_model_for_kbit_training # manual QLoRA setup with kbit quantization\n",
        ")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\") #makes output cleaner\n",
        "from huggingface_hub import notebook_login # Keep login for Llama 3.1\n",
        "\n",
        "CSV_PATH = \"2.4.xlsx\" #uploaded excel file\n",
        "\n",
        "# Column Names (Ensure these EXACTLY match your cleaned CSV/Excel headers)\n",
        "COL_SUBFUNCTION = \"Subfunction\"\n",
        "COL_REQUIREMENTS = \"Requirements\"\n",
        "COL_FAILURE_MODE = \"Potential Failure Mode and descriptions\" # Base name, will be cleane\n",
        "COL_EFFECT_PRIMARY = \"Potential Effect(s) of Failure (primary)\" # Base name\n",
        "COL_EFFECT_SECONDARY = \"Potential Effect(s) of Failure (secondary)\" # Base name\n",
        "COL_SEVERITY = \"Severity\" # target column\n",
        "\n",
        "# Input/Output Columns\n",
        "INPUT_TEXT_COLS = [\n",
        "    COL_SUBFUNCTION, COL_REQUIREMENTS, COL_FAILURE_MODE,\n",
        "    COL_EFFECT_PRIMARY, COL_EFFECT_SECONDARY\n",
        "]\n",
        "\n",
        "\n",
        "#need to fill in empty cells\n",
        "COLS_TO_FORWARD_FILL = [\n",
        "    COL_SUBFUNCTION, COL_REQUIREMENTS, COL_FAILURE_MODE\n",
        "]\n",
        "\n",
        "#want to predict this\n",
        "TARGET_COLUMN = COL_SEVERITY\n",
        "NUM_LABELS = 10 # Severity can only be 1-10\n",
        "\n",
        "# Model Config\n",
        "MODEL_NAME = \"meta-llama/Meta-Llama-3.1-8B-Instruct\" # using 3.2-1B bc 3.1 access has not been approved\n",
        "MAX_SEQ_LENGTH = 512 #for memory management\n",
        "\n",
        "# Training Config\n",
        "OUTPUT_DIR = \"fmea_severity_classifier_llama31_8b_standard_qlora\" # <<< New output dir name\n",
        "LEARNING_RATE = 1e-4      # common QLoRA starting point\n",
        "NUM_EPOCHS = 3            # Train for 3 epochs which is standard for fine tuning\n",
        "# MAX_STEPS = 500         # Alternative to epochs\n",
        "BATCH_SIZE_PER_DEVICE = 1 # may use 4 because model is smaller\n",
        "GRAD_ACCUMULATION_STEPS = 16 # effective batch size of 16\n",
        "LORA_R = 16 #the rank for LoRA matricies 16 is common\n",
        "LORA_ALPHA = 32 #scaling factor for LoRA updates twice or equal to LORA_R is standard\n",
        "LORA_DROPOUT = 0.05 #used for regularization, 0.05 is typical value\n",
        "LOGGING_STEPS = 10 #frequent updates\n",
        "SAVE_STRATEGY = \"epoch\" #when model should be saved\n",
        "EVAL_STRATEGY = \"epoch\"   #when eval should be preformed\n",
        "# -----done configing LoRA------\n",
        "\n",
        "\n",
        "#setup device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #checks if device is uisng cuda GPU, cpu for backup\n",
        "print(f\"Using device: {device}\") #prints out what device is being used\n",
        "if device.type == 'cpu': print(\"Warning: Running on CPU!\") #warning for if using cpu bc cpu training is way too slow\n",
        "\n",
        "# Setup Label Mappings\n",
        "labels_list = [str(i) for i in range(1, 11)] #creates list of strings for severity label from \"1\" to \"10\"\n",
        "id2label = {i: label for i, label in enumerate(labels_list)} #maps integers 0-9 to strings \"1\"-\"10\" needed for classification head and output interpretation\n",
        "label2id = {label: i for i, label in enumerate(labels_list)} #reverse of previous line, maps integers \"1\"-\"10\" to integers 0-9 which is required for training\n",
        "print(f\"id2label mapping: {id2label}\") #print for verification\n",
        "print(f\"label2id mapping: {label2id}\") #print for verification\n",
        "\n",
        "# Check GPU capability for compute dtype in BNBConfig will  use float16 because t4 is the free avalible one\n",
        "compute_dtype = torch.float16\n",
        "if torch.cuda.is_available():\n",
        "    if torch.cuda.get_device_capability()[0] >= 8: # Ampere+ (A100)\n",
        "        compute_dtype = torch.bfloat16\n",
        "        print(\"Compute dtype set to bfloat16 for Ampere+ GPU.\")\n",
        "    else:\n",
        "        print(\"Compute dtype set to float16.\")"
      ],
      "metadata": {
        "id": "W8OzvxnGNf10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f341332f-46b7-48c5-c6f9-c843fc10fc10"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "id2label mapping: {0: '1', 1: '2', 2: '3', 3: '4', 4: '5', 5: '6', 6: '7', 7: '8', 8: '9', 9: '10'}\n",
            "label2id mapping: {'1': 0, '2': 1, '3': 2, '4': 3, '5': 4, '6': 5, '7': 6, '8': 7, '9': 8, '10': 9}\n",
            "Compute dtype set to float16.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load and Preprocess Data\n",
        "#read Excel/CSV, clean header, forward filling, combine text features into a combined \"text\" column, and \"label\" column 0-9\n",
        "#splits data\n",
        "#converts to DatasetDict\n",
        "#normalization off by default\n",
        "\n",
        "print(f\"Loading data from '{CSV_PATH}'...\")\n",
        "\n",
        "\n",
        "#try except block handles reading from excel and csv, and then stores it into pandas dataframe, df\n",
        "try:\n",
        "    try: df = pd.read_excel(CSV_PATH)\n",
        "    except Exception: df = pd.read_csv(CSV_PATH)\n",
        "    #this line will perform string operations to datafram column names, removes new line characters with \\n, replaces multiple spaces with a single space\n",
        "    #removes leading and trailing whitespace\n",
        "    #saves the processed columns to cleaned_columns list\n",
        "    original_columns = df.columns.tolist(); df.columns = df.columns.str.replace('\\n', '', regex=False).str.replace(' +', ' ', regex=True).str.strip(); cleaned_columns = df.columns.tolist()\n",
        "    #maps cleaned columns to original columns\n",
        "    column_map = {clean: orig for clean, orig in zip(cleaned_columns, original_columns)}; print(f\"✅ Loaded {len(df)} rows. Cleaned columns: {cleaned_columns}\")\n",
        "except Exception as e: print(f\"❌ Error loading data: {e}\"); raise #print error if reading from excel or csv produced an exception\n",
        "\n",
        "\n",
        "# Function to get cleaned name robustly (optional, can hardcode if sure)\n",
        "def get_cleaned_name(config_name, df_cols, original_map): # Pass original map too\n",
        "    # Use split() and join() to collapse multiple spaces and remove newlines/strip\n",
        "    cleaned = ' '.join(str(config_name).replace('\\n', '').strip().split()) # same cleaning as before\n",
        "    if cleaned not in df_cols:\n",
        "         original_name = original_map.get(cleaned, config_name) # Try lookup original name if clean fails\n",
        "         print(f\"   Warning: Configured column '{config_name}' -> '{cleaned}' not found after cleaning. Check CSV/Excel headers and config variables.\")\n",
        "         # Fallback to original name might be safer if cleaning leads to mismatch\n",
        "         if original_name in df_cols: return original_name\n",
        "         return config_name # Return original config if neither found\n",
        "    return cleaned\n",
        "\n",
        "# Update configured names based on cleaned names IN THE DATAFRAME\n",
        "# This has all parts of the FMEA table, and uses the funciton to get cleaned names\n",
        "# ceratin cells to forward fill bc they are partialy empty in table\n",
        "COL_SUBFUNCTION = get_cleaned_name(COL_SUBFUNCTION, df.columns, column_map)\n",
        "COL_REQUIREMENTS = get_cleaned_name(COL_REQUIREMENTS, df.columns, column_map)\n",
        "COL_FAILURE_MODE = get_cleaned_name(COL_FAILURE_MODE, df.columns, column_map)\n",
        "COL_EFFECT_PRIMARY = get_cleaned_name(COL_EFFECT_PRIMARY, df.columns, column_map)\n",
        "#COL_EFFECT_SECONDARY = get_cleaned_name(COL_EFFECT_SECONDARY, df.columns, column_map)\n",
        "COL_SEVERITY = get_cleaned_name(COL_SEVERITY, df.columns, column_map)\n",
        "INPUT_TEXT_COLS = [COL_SUBFUNCTION, COL_REQUIREMENTS, COL_FAILURE_MODE, COL_EFFECT_PRIMARY] #COL_EFFECT_SECONDARY\n",
        "COLS_TO_FORWARD_FILL = [COL_SUBFUNCTION, COL_REQUIREMENTS, COL_FAILURE_MODE]\n",
        "TARGET_COLUMN = COL_SEVERITY # Already potentially cleaned\n",
        "all_needed_columns = INPUT_TEXT_COLS + [TARGET_COLUMN]\n",
        "print(f\"   Using effective columns: {all_needed_columns}\")\n",
        "\n",
        "# Verify Columns Exist\n",
        "missing_cols = [col for col in all_needed_columns if col not in df.columns]; #list of columns in all_needed_columns not in df.columns, which is all columns\n",
        "if missing_cols: print(f\"❌ Error: Columns missing: {missing_cols}\"); raise ValueError(\"Missing columns\") #print out result if missing columns is not empty, and raises error\n",
        "\n",
        "# Preprocessing\n",
        "print(\"⏳ Preprocessing data...\")\n",
        "df_selected = df[all_needed_columns].copy() #only takes needed columns\n",
        "print(f\"   Forward filling columns: {COLS_TO_FORWARD_FILL}...\")\n",
        "df_selected[COLS_TO_FORWARD_FILL] = df_selected[COLS_TO_FORWARD_FILL].ffill() #forward fills the columns that need it\n",
        "initial_rows = len(df_selected); df_selected = df_selected.dropna(); final_rows = len(df_selected) #count initial rows with len(), then drops rows empty in any column, counts rows after dropping\n",
        "if initial_rows > final_rows: print(f\"   Dropped {initial_rows - final_rows} rows with NaN values.\") #prints if rows were dropped\n",
        "if final_rows == 0: raise ValueError(\"No data left after NaN drop\") #if everything was dropped, raise an Error\n",
        "\n",
        "# Convert Severity & Validate\n",
        "try:\n",
        "    df_selected[TARGET_COLUMN] = pd.to_numeric(df_selected[TARGET_COLUMN], errors='coerce') #converts values in TARGET_COLUMN into a number type, if can't turn into NaN\n",
        "    df_selected = df_selected.dropna(subset=[TARGET_COLUMN]); df_selected[TARGET_COLUMN] = df_selected[TARGET_COLUMN].astype(int) #removes rows if converted to NaN\n",
        "except Exception as e: print(f\"❌ Error converting Severity: {e}\"); raise\n",
        "#filter out data not in 1-10 range and checks if any data remains\n",
        "initial_rows = len(df_selected); df_selected = df_selected[df_selected[TARGET_COLUMN].between(1, 10)]; final_rows = len(df_selected)\n",
        "#sets df_selected to only have rows with value 1-10, then counts the new ammount of rows\n",
        "if initial_rows > final_rows: print(f\"   Removed {initial_rows - final_rows} rows with Severity outside [1, 10].\") #prints out how many rows were removed\n",
        "if final_rows == 0: raise ValueError(\"No data left with valid Severity (1-10)\")\n",
        "\n",
        "# Combine Text Features\n",
        "def combine_features(row):\n",
        "    text_parts = [] #empty list to store the text\n",
        "    for col in INPUT_TEXT_COLS: value = str(row[col]) if pd.notna(row[col]) else \"\"; clean_col_name = col.split('(')[0].strip(); text_parts.append(f\"{clean_col_name}: {value}\")\n",
        "    #goes through every input column and gets the value if it is not NaN, splits at ( and removes leading and trailing whitespace then adds it to text_parts\n",
        "    return \"\\n\".join(text_parts) #returns all text parts together with new line between them as a input for the model\n",
        "print(\"   Combining input text features into 'text' column...\")\n",
        "df_selected['text'] = df_selected.apply(combine_features, axis=1) #creates new column called text, fills it with the combined features in each row\n",
        "\n",
        "# Prepare Labels (0-9)\n",
        "df_selected['label'] = df_selected[TARGET_COLUMN] - 1 #converts from 1-10 to 0-9 because 0 is the first and populates new column label with those values\n",
        "print(f\"   Created 'label' column (0-9) from '{TARGET_COLUMN}'.\")\n",
        "\n",
        "# Keep only necessary columns\n",
        "df_final = df_selected[['text', 'label']] #just needs the combined text features and the 0-9 severity value\n",
        "\n",
        "# Create Train/Validation Split\n",
        "print(\"⏳ Splitting data...\")\n",
        "train_df, valid_df = train_test_split(df_final, test_size=0.2, random_state=42, stratify=df_final['label']) #80% for training, 20% for validation severity makes sure that severity levels are evenly distributed\n",
        "print(f\"✅ Split complete. Train size: {len(train_df)}, Validation size: {len(valid_df)}\") #prints the number of rows for training and validating\n",
        "\n",
        "# Convert to Hugging Face Datasets\n",
        "train_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n",
        "valid_dataset = Dataset.from_pandas(valid_df, preserve_index=False)\n",
        "raw_datasets = DatasetDict({'train': train_dataset, 'validation': valid_dataset}) #create DatasetDict with train and validation and corresponding datasets which is needed for Trainer\n",
        "print(\"✅ Data prepared and converted to Hugging Face Datasets format.\")\n",
        "print(raw_datasets)\n",
        "\n",
        "#  cleanup\n",
        "import gc; del df, df_selected, df_final, train_df, valid_df; gc.collect() #imports garbage collector, deletes unneeded large pandas dataframes for memory saving, and runs garbage collector for memory"
      ],
      "metadata": {
        "id": "jsvggni9DkTI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "276f5148-e0f1-4a39-ac53-f0b2fbb843b7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from '2.4.xlsx'...\n",
            "✅ Loaded 1923 rows. Cleaned columns: ['Subfunction', 'Requirements', 'Potential Failure Mode and descriptions', 'Potential Effect(s) of Failure (primary)', 'Standardised Statement', 'Severity', 'Confidence']\n",
            "   Using effective columns: ['Subfunction', 'Requirements', 'Potential Failure Mode and descriptions', 'Potential Effect(s) of Failure (primary)', 'Severity']\n",
            "⏳ Preprocessing data...\n",
            "   Forward filling columns: ['Subfunction', 'Requirements', 'Potential Failure Mode and descriptions']...\n",
            "   Combining input text features into 'text' column...\n",
            "   Created 'label' column (0-9) from 'Severity'.\n",
            "⏳ Splitting data...\n",
            "✅ Split complete. Train size: 1538, Validation size: 385\n",
            "✅ Data prepared and converted to Hugging Face Datasets format.\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 1538\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 385\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "212"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Hugging Face Login---\n",
        "print(\"\\nPlease log in to Hugging Face using an Access Token with 'read' permission.\")\n",
        "notebook_login()\n",
        "print(\"✅ Login process initiated.\")\n",
        "# --- End Login ---\n",
        "print(f\"\\n⏳ Loading tokenizer for '{MODEL_NAME}'...\")\n",
        "\n",
        "# Load tokenizer associated with the base model\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME) # Login above handles token\n",
        "    # Set padding token (Llama 3 uses EOS)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        print(f\"   Tokenizer pad_token set to eos_token: {tokenizer.pad_token}\")\n",
        "    print(\"✅ Tokenizer loaded.\")\n",
        "except Exception as e: print(f\"❌ Error loading tokenizer: {e}\"); raise\n",
        "\n",
        "# Load tokenizer associated with the base model\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME) # Login above handles token\n",
        "    # Set padding token (Llama 3 uses EOS)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token #is padding token isnt defined sets the pad token to be same as eos token\n",
        "        print(f\"   Tokenizer pad_token set to eos_token: {tokenizer.pad_token}\")\n",
        "    print(\"✅ Tokenizer loaded.\")\n",
        "except Exception as e: print(f\"❌ Error loading tokenizer: {e}\"); raise\n",
        "\n",
        "# Define tokenization function\n",
        "def tokenize_function(examples):\n",
        "    #this tokenizes the \"text\" column and truncation=true tells it to cut off the at the assigned legnth\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=MAX_SEQ_LENGTH, padding=False)\n",
        "\n",
        "print(\"⏳ Tokenizing datasets...\")\n",
        "#tokenizes entire training and validation data sets\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "#assigns the result of map() to tokenized datasets\n",
        "#map applies the tokenize funciton in batches\n",
        "#removes \"text\" column from tokenized datasets\n",
        "print(\"✅ Datasets tokenized.\")\n",
        "print(tokenized_datasets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 839,
          "referenced_widgets": [
            "5c4853a05da041b0a0bc1a0a5063d383",
            "e8738f16d2dc445789e23c260fe39d03",
            "b836fcd876cc43008532a4698ef340ff",
            "1a8be729065c48968b2bfef5de3cc541",
            "8cbb9ba3c4c843fe921bdc764d7744d8",
            "0079dd9f60c641de99c36a2a31ea8e29",
            "d188c2ef85fa4e5fb60aecfb9c4a44d7",
            "e1a0d955d57f4004b0f12223d63aa335",
            "482c46f10dc54644bab78ce150d9e8ae",
            "32693348a7414f67aa63e9590cffe719",
            "f21bb392714547c4805a9f9f5170df1d",
            "6ec77183777f423d928fb9ab37c85c80",
            "05bcc2d2b1b3473cba1ce56a83203c57",
            "79fb42f9e04b4c5981594a848aa87dcc",
            "14ae1aa481ff4640bf6d84394596043f",
            "0c9fcb6fc5e74859836f1820ab14e7d3",
            "d4de73971cc342be82926b7f6dc8ce4c",
            "8654a33c5ae547f2a04c6051baab3dcd",
            "d74729e7a4954e0389a0b331deeefb21",
            "258c5e137e114a1fa4c080f4865a2449",
            "5200a4039bea4651acd504a7fc4f116c",
            "19b0fade90c34f6eb5fbcc52d02f1902",
            "99080334ed4e43948133ce21a0c4aabe"
          ]
        },
        "id": "fc-fpWD0-0va",
        "outputId": "c57d5cb3-2b7b-4033-afa9-55ecde7f1aa9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Please log in to Hugging Face using an Access Token with 'read' permission.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c4853a05da041b0a0bc1a0a5063d383"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Login process initiated.\n",
            "\n",
            "⏳ Loading tokenizer for 'meta-llama/Meta-Llama-3.1-8B-Instruct'...\n",
            "❌ Error loading tokenizer: We couldn't connect to 'https://huggingface.co' to load the files, and couldn't find them in the cached files.\n",
            "Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "We couldn't connect to 'https://huggingface.co' to load the files, and couldn't find them in the cached files.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1532\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1533\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1534\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1449\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1450\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1451\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    287\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_backoff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry_on_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry_on_status_codes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m429\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    472\u001b[0m             )\n\u001b[0;32m--> 473\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHfHubHTTPError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHfHubHTTPError\u001b[0m: 403 Forbidden: None.\nCannot access content at: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\nMake sure your token has the correct permissions.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mLocalEntryNotFoundError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0;31m# This is slightly better for only 1 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m             hf_hub_download(\n\u001b[0m\u001b[1;32m    471\u001b[0m                 \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1007\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m   1009\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1114\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1115\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1647\u001b[0m         \u001b[0;31m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1648\u001b[0;31m         raise LocalEntryNotFoundError(\n\u001b[0m\u001b[1;32m   1649\u001b[0m             \u001b[0;34m\"An error happened while trying to locate the file on the Hub and we cannot find the requested files\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLocalEntryNotFoundError\u001b[0m: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-3377886694>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Load tokenizer associated with the base model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Login above handles token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Set padding token (Llama 3 uses EOS)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    968\u001b[0m                     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m                     config = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m    971\u001b[0m                         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0mcode_revision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"code_revision\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1153\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1154\u001b[0m         \u001b[0mhas_remote_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"auto_map\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"AutoConfig\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auto_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m         \u001b[0mhas_local_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;31m# Get config dict associated with the base config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    652\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m                 \u001b[0;31m# Load from local folder or from cache or download from model Hub and cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    655\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m                     \u001b[0mconfiguration_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \"\"\"\n\u001b[0;32m--> 312\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0;31m# even when `local_files_only` is True, in which case raising for connections errors only would not make sense)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0m_raise_exceptions_for_missing_entries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m                 raise OSError(\n\u001b[0m\u001b[1;32m    544\u001b[0m                     \u001b[0;34mf\"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load the files, and couldn't find them in the\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m                     \u001b[0;34mf\" cached files.\\nCheckout your internet connection or see how to run the library in offline mode at\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: We couldn't connect to 'https://huggingface.co' to load the files, and couldn't find them in the cached files.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Llama Model\n",
        "\n",
        "\n",
        "print(\"⚙️ Defining 4-bit quantization config (BitsAndBytesConfig)...\")\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, #important setting that enables quantization\n",
        "    bnb_4bit_quant_type=\"nf4\", #type of quanitzation to use\n",
        "    bnb_4bit_compute_dtype=compute_dtype, # Determined in Cell 3 based on GPU\n",
        "    bnb_4bit_use_double_quant=True, #quantizes already quantized data for memory savings\n",
        ")\n",
        "print(\"✅ Quantization config defined.\")\n",
        "\n",
        "# --- Load Base Model with Quantization ---\n",
        "print(f\"⏳ Loading base model '{MODEL_NAME}' for Sequence Classification with 4-bit quantization...\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME, #name of the model\n",
        "    quantization_config=bnb_config, #passed config from earlier to tell the model to load with 4-bit quantization\n",
        "    device_map=\"auto\", # Use \"auto\" for standard HF, should work better on A100\n",
        "    # device_map = {\"\": 0}, # Use explicit mapping if \"auto\" causes issues\n",
        "    num_labels=NUM_LABELS, #number of output lables, 10 for this case\n",
        "    id2label=id2label, #dictionary mapping 0-9 to \"1\"-\"10\"\n",
        "    label2id=label2id, #dicionary map the other way\n",
        "    # ignore_mismatched_sizes=True # Try uncommenting if size mismatch error occurs\n",
        ")\n",
        "print(\"✅ Base model loaded with quantization.\")\n",
        "\n",
        "# Set pad token ID in model config if tokenizer has one (important!)\n",
        "if tokenizer.pad_token_id is not None: #checks for a padding token\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id #if there is a padding token, this sets it in the config\n",
        "    print(f\"Model pad_token_id set to: {model.config.pad_token_id}\")\n",
        "\n",
        "# --- Prepare Model for K-bit Training & Apply LoRA using PEFT ---\n",
        "print(\"⚙️ Preparing model for K-bit training and defining LoRA config (PEFT)...\")\n",
        "model.gradient_checkpointing_enable() # Often needed for K-bit training, trades memory for computation\n",
        "model = prepare_model_for_kbit_training(model) #further prepares model\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_R, #sets lora rank to previously defined\n",
        "    lora_alpha=LORA_ALPHA, #scaling factor, previously defined\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", # Standard Llama 3 targets\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=LORA_DROPOUT, #sets dropout probability to previously defined\n",
        "    bias=\"none\", #number of bias terms in LoRA matricies\n",
        "    task_type=TaskType.SEQ_CLS, # Specify Sequence Classification task\n",
        ")\n",
        "print(\"✅ LoRA configuration defined.\")\n",
        "\n",
        "print(\"⚡️ Applying LoRA adapter to the model using PEFT...\")\n",
        "model = get_peft_model(model, lora_config) # Standard PEFT function, returns new model with LoRA adapters\n",
        "print(\"✅ LoRA adapter applied.\")\n",
        "model.print_trainable_parameters() #prints summary of the model's paramaters\n",
        "\n",
        "# --- Data Collator ---\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer) #pads things to maximum legnth\n",
        "print(\"✅ Data collator created.\")"
      ],
      "metadata": {
        "id": "NB4THQKd6h8m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275,
          "referenced_widgets": [
            "7cf3d13d08ad46268f23ab5749016790",
            "d35b0361011447cab5594587ac72a81a",
            "5abe9f12e71c45eab0b7058dfa41ac54",
            "b238dea034a8403aa7f063c523e9d2a6",
            "b900e00fce744e64b9227bb63e73a3da",
            "895c7bc8f9c04aaab5890331654a1414",
            "c0ae4793a3764fd0a82a215b4db0266b",
            "03f839a808334cee97e63fda2ea710b6",
            "94ce2b6d236a431494ac1e068d0db533",
            "f1d111be47d1451ea38465e2d7a532a1",
            "7f3f79e352d84f90b9e877cce1091d5d"
          ]
        },
        "outputId": "76155daf-fda8-4912-e27c-b547a54da3f6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚙️ Defining 4-bit quantization config (BitsAndBytesConfig)...\n",
            "✅ Quantization config defined.\n",
            "⏳ Loading base model 'meta-llama/Llama-3.2-1B' for Sequence Classification with 4-bit quantization...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7cf3d13d08ad46268f23ab5749016790"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Base model loaded with quantization.\n",
            "Model pad_token_id set to: 128001\n",
            "⚙️ Preparing model for K-bit training and defining LoRA config (PEFT)...\n",
            "✅ LoRA configuration defined.\n",
            "⚡️ Applying LoRA adapter to the model using PEFT...\n",
            "✅ LoRA adapter applied.\n",
            "trainable params: 11,292,672 || all params: 1,247,127,552 || trainable%: 0.9055\n",
            "✅ Data collator created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from transformers import TrainingArguments, Trainer # Ensure these are imported\n",
        "import torch # Ensure torch is imported\n",
        "\n",
        "# --- Define Compute Metrics Function ---\n",
        "# Keep this function as it's needed for manual evaluation later\n",
        "def compute_metrics(eval_pred): #function that takes in a tuple with arguments of the prediction and true label\n",
        "    predictions, labels = eval_pred; preds = np.argmax(predictions, axis=1) #unpacks tuple, and calculates predicted class index\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=0) #computers precision, recall, and f1 scores\n",
        "    acc = accuracy_score(labels, preds); return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}\n",
        "    #compares accuracy by comparing true labels and prediction\n",
        "\n",
        "# --- Define Training Arguments (Workaround Applied) ---\n",
        "print(\"⚙️ Setting Training Arguments (evaluation_strategy workaround)...\")\n",
        "\n",
        "# Check GPU capability for fp16/bf16 (should be done in Cell 3, but check again is ok)\n",
        "bf16_supported = False\n",
        "fp16_enabled = False\n",
        "if torch.cuda.is_available():\n",
        "    if torch.cuda.get_device_capability()[0] >= 8: # Ampere+ (A100, etc.)\n",
        "        bf16_supported = True\n",
        "        print(\"   Setting bf16=True for Ampere+ GPU.\")\n",
        "    else: # T4, V100, etc.\n",
        "        fp16_enabled = True\n",
        "        print(\"   Setting fp16=True for non-Ampere GPU.\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR + \"_chkpts\", # Use OUTPUT_DIR from Cell 3 config\n",
        "    # --- Training Duration & Batching (Use config from Cell 3) ---\n",
        "    num_train_epochs = NUM_EPOCHS,\n",
        "    # max_steps = MAX_STEPS, # Alternatively use max_steps\n",
        "    per_device_train_batch_size=BATCH_SIZE_PER_DEVICE, #from cell 3\n",
        "    gradient_accumulation_steps=GRAD_ACCUMULATION_STEPS, #from cell 3\n",
        "    learning_rate=LEARNING_RATE, #from cell 3\n",
        "\n",
        "    # --- Optimizer & Precision ---\n",
        "    optim=\"paged_adamw_8bit\", # Recommended 8-bit optimizer for QLoRA\n",
        "    fp16=fp16_enabled,        # Enable based on GPU check\n",
        "    bf16=bf16_supported,      # Enable based on GPU check\n",
        "\n",
        "    # --- Logging & Saving ---\n",
        "    logging_strategy=\"steps\", #determines when the log info\n",
        "    logging_steps=LOGGING_STEPS, #from cell 3\n",
        "    save_strategy=SAVE_STRATEGY,     # e.g., \"epoch\" or \"steps\"\n",
        "    # save_steps = SAVE_STEPS,      # Use if save_strategy=\"steps\"\n",
        "    save_total_limit=1,          # Optional: keep only last/best checkpoint\n",
        "\n",
        "    # --- WORKAROUND APPLIED ---\n",
        "    # evaluation_strategy=\"epoch\", # <<< COMMENTED OUT / REMOVED\n",
        "    # load_best_model_at_end=True, # <<< MUST be False if not evaluating during training\n",
        "    # metric_for_best_model=\"f1\",  # <<< Comment out / remove\n",
        "    load_best_model_at_end=False,  # Explicitly set to False\n",
        "\n",
        "    # --- Other Args ---\n",
        "    seed=42, #sets the seed for repoducibility\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=True, # Safe if Cell 5 removed 'text' column\n",
        "    gradient_checkpointing=True, # Recommended for standard QLoRA memory saving\n",
        "    gradient_checkpointing_kwargs={'use_reentrant':False},\n",
        ")\n",
        "\n",
        "# --- Create Trainer ---\n",
        "print(\"⚙️ Creating Trainer...\")\n",
        "# Ensure model, tokenized_datasets, tokenizer, data_collator exist from previous cells\n",
        "try:\n",
        "    #creates trainer with all of the configs from earlier\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_datasets[\"train\"],\n",
        "        eval_dataset=tokenized_datasets[\"validation\"], # Keep for manual eval\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics, # Keep for manual eval\n",
        "    )\n",
        "    print(\"✅ Trainer created.\")\n",
        "except NameError as ne:\n",
        "    print(f\"❌ NameError: A required object (model, dataset, etc.) not found: {ne}\")\n",
        "    print(\"   Please ensure Cells 3, 4, 5, 6 ran successfully.\")\n",
        "    raise\n",
        "except Exception as e:\n",
        "    print(f\"❌ Unexpected error creating Trainer: {e}\")\n",
        "    raise\n",
        "\n",
        "# --- Start Training ---\n",
        "print(f\"\\n🚀🚀🚀 Starting Standard QLoRA Fine-tuning! 🚀🚀🚀\")\n",
        "try:\n",
        "    train_result = trainer.train() # Train the model\n",
        "    print(\"\\n✅✅✅ Training finished! ✅✅✅\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ An error occurred during trainer.train(): {e}\")\n",
        "    raise\n",
        "\n",
        "# --- !! Manually Evaluate Model AFTER Training !! ---\n",
        "print(\"\\n🧪 Evaluating model after training has completed...\")\n",
        "try:\n",
        "    eval_results = trainer.evaluate(eval_dataset=tokenized_datasets[\"validation\"]) #evaluates model based on validation dataset\n",
        "    print(\"\\n📊 Final Validation Set Evaluation Results (Manual Trigger):\")\n",
        "    print(eval_results)\n",
        "    trainer.log_metrics(\"eval_manual\", eval_results)\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error during manual evaluation: {e}\")\n",
        "\n",
        "# --- Save Final Model State ---\n",
        "# Note: Saves the model state at the END of training.\n",
        "print(f\"\\n💾 Saving final trained model adapter & tokenizer to '{OUTPUT_DIR}'...\")\n",
        "try:\n",
        "    trainer.save_model(OUTPUT_DIR)\n",
        "    if 'tokenizer' in locals() and tokenizer is not None: # Save tokenizer if available\n",
        "         tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "    print(f\"✅ Final model adapter and tokenizer saved to '{OUTPUT_DIR}'.\")\n",
        "except Exception as e:\n",
        "     print(f\"❌ Error saving model/tokenizer: {e}\")\n",
        "\n",
        "# --- Optional: Clean up GPU memory ---\n",
        "import gc\n",
        "# Add del statements for objects no longer needed\n",
        "# Example: del model, trainer, tokenized_datasets, raw_datasets\n",
        "gc.collect()\n",
        "if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "print(\"\\n🧹 Training cell GPU memory cache potentially cleared.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5zsY0wquZtXX",
        "outputId": "84cb0080-bf3c-4333-dd07-2878c8c33f3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⚙️ Setting Training Arguments (evaluation_strategy workaround)...\n",
            "   Setting fp16=True for non-Ampere GPU.\n",
            "⚙️ Creating Trainer...\n",
            "✅ Trainer created.\n",
            "\n",
            "🚀🚀🚀 Starting Standard QLoRA Fine-tuning! 🚀🚀🚀\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='246' max='291' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [246/291 24:20 < 04:29, 0.17 it/s, Epoch 2.53/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.704100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.725800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.744000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.671800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.649100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.775400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.804500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.766400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.781300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.433000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.669600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.554900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.577200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.621400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.510700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.600600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>1.556300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>1.651700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>1.563900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.400500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>1.381300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>1.444700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>1.416300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>1.530900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='256' max='291' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [256/291 25:20 < 03:29, 0.17 it/s, Epoch 2.63/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.704100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.725800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.744000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.671800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.649100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.775400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.804500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.766400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.781300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.433000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.669600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.554900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.577200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.621400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.510700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.600600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>1.556300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>1.651700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>1.563900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.400500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>1.381300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>1.444700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>1.416300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>1.530900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.474500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Optional Download Code Block (Run in new cell after Cell 7) ---\n",
        "import shutil\n",
        "from google.colab import files\n",
        "import os\n",
        "import time\n",
        "\n",
        "folder_to_download = \"fmea_severity_classifier_llama31_8b_standard_qlora\" # used later\n",
        "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "zip_filename = f\"{folder_to_download}_{timestamp}.zip\"\n",
        "\n",
        "print(f\"\\n📦 Preparing folder '{folder_to_download}' for download...\")\n",
        "try:\n",
        "    if os.path.exists(folder_to_download):\n",
        "        print(f\"   Zipping folder to '{zip_filename}'...\")\n",
        "        shutil.make_archive(folder_to_download, 'zip', folder_to_download)\n",
        "        print(f\"   Zipping complete.\")\n",
        "        print(f\"⬇️ Triggering browser download for '{zip_filename}'...\")\n",
        "        files.download(zip_filename) # Trigger download\n",
        "        print(f\"✅ Download initiated. Check your browser.\")\n",
        "    else:\n",
        "        print(f\"❌ Error: Output directory '{folder_to_download}' not found. Cannot download.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ An error occurred during zipping or downloading: {e}\")\n",
        "# --- End Download Code Block ---"
      ],
      "metadata": {
        "id": "ZG5H8td4c7Xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on Validation Set\n",
        "# Reloads model if needed and prints report\n",
        "\n",
        "\n",
        "try:\n",
        "    trainer # Check if trainer from Cell 7 exists\n",
        "    trainer_to_use = trainer\n",
        "    # Ensure dataset and mappings are accessible\n",
        "    dataset_to_eval = tokenized_datasets[\"validation\"]\n",
        "    id2label_eval = id2label\n",
        "    NUM_LABELS_EVAL = NUM_LABELS\n",
        "    print(\"Using existing trainer object for prediction.\")\n",
        "except NameError:\n",
        "    print(\"Trainer object not found. Loading model from disk (Standard QLoRA)...\")\n",
        "    from transformers import AutoTokenizer, AutoModelForSequenceClassification, BitsAndBytesConfig, Trainer, TrainingArguments\n",
        "    from peft import PeftModel\n",
        "    import torch\n",
        "\n",
        "    ADAPTER_PATH_EVAL = OUTPUT_DIR # Use OUTPUT_DIR from Cell 3\n",
        "    MODEL_NAME_EVAL = MODEL_NAME # Use MODEL_NAME from Cell 3\n",
        "    # Reload tokenizer\n",
        "    tokenizer_eval = AutoTokenizer.from_pretrained(ADAPTER_PATH_EVAL) #loads from saved files\n",
        "    if tokenizer_eval.pad_token is None: tokenizer_eval.pad_token = tokenizer_eval.eos_token #same setup as training\n",
        "    # Reload base model with quantization config\n",
        "    compute_dtype_eval = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else torch.float16\n",
        "    bnb_config_eval = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=compute_dtype_eval, bnb_4bit_use_double_quant=True)\n",
        "    base_model_eval = AutoModelForSequenceClassification.from_pretrained(\n",
        "        MODEL_NAME_EVAL, quantization_config=bnb_config_eval, device_map=\"auto\",\n",
        "        num_labels=NUM_LABELS, id2label=id2label, label2id=label2id\n",
        "    )\n",
        "    if base_model_eval.config.pad_token_id is None: base_model_eval.config.pad_token_id = tokenizer_eval.pad_token_id\n",
        "    #sets padding token id if not already set\n",
        "\n",
        "    # Load adapter\n",
        "    model_eval = PeftModel.from_pretrained(base_model_eval, ADAPTER_PATH_EVAL)\n",
        "    model_eval.eval()\n",
        "    print(\"Model reloaded from disk.\")\n",
        "    # Create dummy trainer for .predict()\n",
        "    dummy_args = TrainingArguments(output_dir=\"./eval_temp_std\", report_to=\"none\", device=model_eval.device)\n",
        "    eval_trainer = Trainer(model=model_eval, args=dummy_args, tokenizer=tokenizer_eval)\n",
        "    trainer_to_use = eval_trainer\n",
        "    # Need to re-run tokenization if 'tokenized_datasets' not available\n",
        "    # Assuming it's available or re-run Cell 5\n",
        "    dataset_to_eval = tokenized_datasets[\"validation\"]\n",
        "    id2label_eval = id2label; NUM_LABELS_EVAL = NUM_LABELS\n",
        "\n",
        "# Get predictions\n",
        "predictions_output = trainer_to_use.predict(dataset_to_eval) #pass in eval dataset and runs the model\n",
        "y_true = predictions_output.label_ids #holds the true labels\n",
        "y_pred = np.argmax(predictions_output.predictions, axis=1) #calculates predicted label based on raw outputs\n",
        "\n",
        "# Generate report using label names (\"1\" to \"10\")\n",
        "# Define the full range of expected label indices (0 to 9)\n",
        "expected_labels = list(range(NUM_LABELS_EVAL)) # Should be [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "# Ensure target_names correspond to these expected labels\n",
        "target_names = [id2label_eval[i] for i in expected_labels]\n",
        "\n",
        "# Call classification_report with the 'labels' parameter specified\n",
        "report = classification_report(\n",
        "    y_true,\n",
        "    y_pred,\n",
        "    labels=expected_labels, # <<< Tell function to report these labels\n",
        "    target_names=target_names,\n",
        "    digits=4,\n",
        "    zero_division=0\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "hvZgIcYOdTJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ───────────────────────────────────────────────────────────\n",
        "# Cell 8A: Run Prediction & Inspect Results\n",
        "# ───────────────────────────────────────────────────────────\n",
        "import numpy as np\n",
        "import pandas as pd # Needed for unique check potentially\n",
        "# Make sure necessary libraries/objects from previous cells are loaded\n",
        "# (Trainer, tokenized_datasets, id2label, NUM_LABELS, etc.)\n",
        "print(\"\\n📋 Preparing for detailed classification report...\")\n",
        "# --- Logic to find or reload trainer and data ---\n",
        "try:\n",
        "    trainer # Check if trainer from Cell 7 exists\n",
        "    # Ensure needed variables are accessible\n",
        "    if 'trainer_to_use' not in locals(): trainer_to_use = trainer\n",
        "    if 'dataset_to_eval' not in locals(): dataset_to_eval = tokenized_datasets[\"validation\"]\n",
        "    if 'id2label_eval' not in locals(): id2label_eval = id2label\n",
        "    if 'NUM_LABELS_EVAL' not in locals(): NUM_LABELS_EVAL = NUM_LABELS\n",
        "    print(\"Using existing trainer object and data for prediction.\")\n",
        "except NameError:\n",
        "    print(\"Trainer object or other necessary variables not found. Attempting to reload model...\")\n",
        "    # Include the reloading logic from your original Cell 8 here if needed\n",
        "    # Make sure ADAPTER_PATH_EVAL, MODEL_NAME_EVAL etc. are defined correctly based on Cell 3/7\n",
        "    # For simplicity, assuming Cell 7 objects still exist. Add reloading if required.\n",
        "    print(\"Error: Cannot proceed without trainer object or reloaded model. Please ensure Cell 7 ran or add reloading code.\")\n",
        "    raise NameError(\"Trainer not found and reloading logic missing/failed.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error setting up for prediction: {e}\")\n",
        "    raise\n",
        "# --- End finding trainer/data ---\n",
        "\n",
        "# --- Get Predictions ---\n",
        "print(f\"\\n⏳ Running prediction on validation set ({len(dataset_to_eval)} samples)...\")\n",
        "try:\n",
        "    predictions_output = trainer_to_use.predict(dataset_to_eval) #passes the model in, and runs forward pass of model without back prop\n",
        "    print(\"✅ trainer.predict() finished successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error during trainer.predict(): {e}\")\n",
        "    raise # Stop if prediction fails\n",
        "\n",
        "# --- Inspect Prediction Outputs ---\n",
        "try:\n",
        "    y_true = predictions_output.label_ids #true labels\n",
        "    y_pred = np.argmax(predictions_output.predictions, axis=1) #predicted labels by taking raw outputs\n",
        "    print(\"\\n--- Prediction Output Inspection ---\")\n",
        "    print(f\"Shape of y_true (true labels): {y_true.shape}\")\n",
        "    print(f\"Shape of y_pred (predicted labels): {y_pred.shape}\")\n",
        "    print(f\"Unique true labels found in validation set: {np.unique(y_true)}\")\n",
        "    print(f\"Unique predicted labels by the model: {np.unique(y_pred)}\")\n",
        "    print(f\"Data type of y_true: {y_true.dtype}\")\n",
        "    print(f\"Data type of y_pred: {y_pred.dtype}\")\n",
        "    print(f\"Any NaN in y_true?: {np.isnan(y_true).any()}\")\n",
        "    # y_pred from argmax should not contain NaN unless logits were NaN\n",
        "    print(\"------------------------------------\")\n",
        "    print(\"\\n✅ Inspection complete. If shapes look correct and labels are in range [0-9], proceed to Cell 8B.\")\n",
        "\n",
        "    # Make variables available for the next cell (Colab usually does this automatically)\n",
        "    # If issues arise, you might need to declare them global, but try without first.\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error during result inspection: {e}\")\n",
        "    raise\n",
        "# --- End Inspection ---\n",
        "\n",
        "# NOTE: We stop here and run the report generation in the next cell (Cell 8B)\n",
        "# %%"
      ],
      "metadata": {
        "id": "qepGIxsVi5CZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate and Print Report\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "# Make sure necessary variables exist from previous cell's execution\n",
        "# (y_true, y_pred, id2label_eval, NUM_LABELS_EVAL)\n",
        "\n",
        "print(\"\\n⚙️ Preparing to generate classification report...\")\n",
        "\n",
        "try:\n",
        "    # Check if needed variables exist\n",
        "    y_true\n",
        "    y_pred\n",
        "    id2label_eval\n",
        "    NUM_LABELS_EVAL\n",
        "\n",
        "    # Define the full range of expected label indices (0 to 9)\n",
        "    expected_labels = list(range(NUM_LABELS_EVAL))\n",
        "    # Ensure target_names correspond to these expected labels\n",
        "    target_names = [id2label_eval[i] for i in expected_labels]\n",
        "\n",
        "    print(\"⏳ Calculating classification report...\")\n",
        "    # Call classification_report with the 'labels' parameter specified\n",
        "    report = classification_report(\n",
        "        y_true,\n",
        "        y_pred,\n",
        "        labels=expected_labels, # Tell function all expected labels\n",
        "        target_names=target_names,\n",
        "        digits=4,\n",
        "        zero_division=0 # Handle labels with no predictions/support\n",
        "    )\n",
        "\n",
        "    print(\"\\n✅ Report calculation finished.\")\n",
        "    print(\"\\n📋 Classification Report:\\n\")\n",
        "    print(report) # <<< Print the calculated report\n",
        "\n",
        "except NameError as ne:\n",
        "     print(f\"❌ NameError: A required variable (y_true, y_pred, etc.) is missing: {ne}\")\n",
        "     print(\"   Please ensure Cell 8A ran successfully first.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error during classification_report generation or printing: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc() # Print detailed traceback for errors here"
      ],
      "metadata": {
        "id": "iW6-9qI3kNA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ───────────────────────────────────────────────────────────\n",
        "# Cell 9: Manual Prediction Function (Standard QLoRA for Llama 3.1)\n",
        "# ───────────────────────────────────────────────────────────\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# import re # Not needed for classification output\n",
        "\n",
        "# --- Configuration ---\n",
        "MODEL_NAME = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "ADAPTER_PATH = \"fmea_severity_classifier_llama31_8b_standard_qlora\" # <<< same path at cell 7\n",
        "MAX_SEQ_LENGTH = 512\n",
        "# Define input columns EXACTLY as used in training (Cell 4)\n",
        "# Assumes these were correctly defined/cleaned before\n",
        "COL_SUBFUNCTION = \"Subfunction\"; COL_REQUIREMENTS = \"Requirements\"; COL_FAILURE_MODE = \"Potential Failure Mode and descriptions\"\n",
        "COL_EFFECT_PRIMARY = \"Potential Effect(s) of Failure (primary)\"; #COL_EFFECT_SECONDARY = \"Potential Effect(s) of Failure (secondary)\" #cant use right now\n",
        "INPUT_COLS_MANUAL = [COL_SUBFUNCTION, COL_REQUIREMENTS, COL_FAILURE_MODE, COL_EFFECT_PRIMARY] #, COL_EFFECT_SECONDARY]\n",
        "NUM_LABELS = 10\n",
        "id2label = {i: str(i+1) for i in range(NUM_LABELS)}\n",
        "# --- End Configuration ---\n",
        "\n",
        "# --- Load Fine-tuned Model and Tokenizer ---\n",
        "# Ensure this loading logic runs successfully before prediction\n",
        "print(\"⏳ Loading fine-tuned Llama 3.2 model for manual prediction (Standard QLoRA)...\")\n",
        "try:\n",
        "    # Define quantization config again\n",
        "    compute_dtype_pred = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else torch.float16\n",
        "    bnb_config_pred = BitsAndBytesConfig(\n",
        "        load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=compute_dtype_pred, bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "    # Load base model with quantization, similar as before\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        quantization_config=bnb_config_pred,\n",
        "        device_map=\"auto\", # Or {\"\": 0}\n",
        "        num_labels=NUM_LABELS,\n",
        "        id2label=id2label,\n",
        "        label2id={v: k for k, v in id2label.items()},\n",
        "        # token = \"hf_...\" # Add if login via notebook_login() didn't persist\n",
        "    )\n",
        "    # Load the tokenizer associated with the saved adapter/base\n",
        "    tokenizer = AutoTokenizer.from_pretrained(ADAPTER_PATH) # Load from adapter path\n",
        "    if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
        "    if model.config.pad_token_id is None: model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    # Load the LoRA adapter onto the base model\n",
        "    print(f\"   Applying LoRA adapter from {ADAPTER_PATH}...\")\n",
        "    model = PeftModel.from_pretrained(model, ADAPTER_PATH)\n",
        "    model.eval()\n",
        "    device = model.device\n",
        "    print(f\"✅ Model and tokenizer loaded on device: {device}\")\n",
        "\n",
        "except Exception as e: print(f\"❌ Error loading model/adapter: {e}\"); raise\n",
        "# --- End Model Loading ---\n",
        "\n",
        "# --- Define Prediction Function ---\n",
        "def predict_fmea_severity_final(**kwargs):\n",
        "    \"\"\" Takes keyword arguments for FMEA input features and predicts Severity (1-10). \"\"\"\n",
        "    # Build the input text string\n",
        "    text_parts = []; missing_args = []\n",
        "    for col in INPUT_COLS_MANUAL:\n",
        "        value = kwargs.get(col); value = str(value) if pd.notna(value) else \"\"\n",
        "        clean_col_name = col.split('(')[0].strip(); text_parts.append(f\"{clean_col_name}: {value}\")\n",
        "    combined_text = \"\\n\".join(text_parts)\n",
        "    print(f\"--- Input Text for Model ---\\n{combined_text}\\n--------------------------\")\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer([combined_text], return_tensors=\"pt\", truncation=True, padding=True, max_length=MAX_SEQ_LENGTH).to(device)\n",
        "\n",
        "    # Predict\n",
        "    print(\"⏳ Predicting severity...\")\n",
        "    with torch.no_grad(): outputs = model(**inputs); logits = outputs.logits\n",
        "    predicted_class_id = torch.argmax(logits, dim=-1).item()\n",
        "    predicted_severity = id2label.get(predicted_class_id, \"Unknown\") # Use mapping\n",
        "\n",
        "    print(f\"✅ Predicted Severity (1-10): {predicted_severity}\")\n",
        "    return predicted_severity\n",
        "\n",
        "# --- Example Usage (Using User Provided Scenarios)\n",
        "print(\"\\n--- Manual Prediction Examples (User Provided) ---\")\n",
        "\n",
        "# Example 1: Emergency Maneuvers\n",
        "print(\"--- Predicting User Example 1 ---\")\n",
        "pred_user_1 = predict_fmea_severity_final(\n",
        "    # Use **{} for keys with spaces/symbols, ensure keys match cleaned column names\n",
        "    **{COL_SUBFUNCTION: \"Emergency Maneuvers\",\n",
        "       COL_REQUIREMENTS: \"Manage safe operations by reacting to sudden braking or lane changes by other vehicles or objects\",\n",
        "       COL_FAILURE_MODE: \"No Function (The autonomous truck fails to detect or react appropriately [brake, steer] to sudden braking, lane changes by other vehicles, or objects appearing in the path, thereby failing to manage safe operations during emergency scenarios.)\",\n",
        "       COL_EFFECT_PRIMARY: \"AV fails to apply required emergency braking\",\n",
        "       COL_EFFECT_SECONDARY: \"results in traffic citation\"}\n",
        ")\n",
        "print(f\"Predicted Severity for User Example 1: {pred_user_1}\\n\")\n",
        "\n",
        "# Example 2: Move For Disabled/Stopped Vehicles\n",
        "print(\"--- Predicting User Example 2 ---\")\n",
        "pred_user_2 = predict_fmea_severity_final(\n",
        "    **{COL_SUBFUNCTION: \"Move For Disabled/Stopped Vehicles\",\n",
        "       COL_REQUIREMENTS: \"Manage safe operations by operating appropriately to disabled or emergency vehicles that are stationary or stopped on the road or on the shoulder.\",\n",
        "       COL_FAILURE_MODE: \"No Function (The autonomous truck fails to detect a stationary disabled/emergency vehicle or fails to execute required safe operations like reducing speed, changing lanes [moving over], or providing adequate lateral clearance, thereby failing to manage safe operations.)\",\n",
        "       COL_EFFECT_PRIMARY: \"AV fails to reduce speed when approaching stationary vehicle/personnel\",\n",
        "       COL_EFFECT_SECONDARY: \"results in traffic citation\"}\n",
        ")\n",
        "print(f\"Predicted Severity for User Example 2: {pred_user_2}\\n\")\n",
        "\n",
        "# --- End Example Usage ---"
      ],
      "metadata": {
        "id": "i7U625uxkoKp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}