{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMx6H9QyG8HMyGdSRHvJN7t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheTigerHub/UTD-Summer-2025/blob/main/UTD2025_FMEA_Severity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgwOMXjswnBk",
        "outputId": "36bd8930-54a8-498a-8d55-3b1b378f1fbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "installing hugging face libraries for QLoRA\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hinstalled accelerate\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hinstalled peft\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hinstalled bitsandbytes\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hinstalled transformers\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0minstalled datasets\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hinstalled scikit-learn\n",
            "installed pandas\n",
            "installed openpyxl\n",
            "\n",
            " libraries installed\n"
          ]
        }
      ],
      "source": [
        "print (\"installing hugging face libraries for QLoRA\")\n",
        "\n",
        "!pip install -q accelerate==0.30.1 #hardware acceleration\n",
        "print (\"installed accelerate\")\n",
        "!pip install -q peft==0.11.1 #peft is parameter effecient fine tuning which allows LoRA and QLoRA\n",
        "print (\"installed peft\")\n",
        "!pip install -q bitsandbytes==0.43.1 #bits and bytes helps with quantization\n",
        "print(\"installed bitsandbytes\")\n",
        "!pip install -q transformers==4.41.1 #hugging face library for pre-trained models, tokenizers\n",
        "print(\"installed transformers\")\n",
        "!pip install -q datasets==2.19.0 #datasets is used for loading, processing, and managing datasets\n",
        "print(\"installed datasets\")\n",
        "!pip install -q scikit-learn==1.4.2 # scikit-learn for data splitting\n",
        "print(\"installed scikit-learn\")\n",
        "!pip install -q pandas #used for data manipulation and analysis\n",
        "print(\"installed pandas\")\n",
        "!pip install -q openpyxl #read excel\n",
        "print(\"installed openpyxl\")\n",
        "\n",
        "print (\"\\n libraries installed\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"installing hugging face libraries for QLoRA\")\n",
        "\n",
        "!pip install -q \"peft[bnb]\" --upgrade #installs peft pacakge with bnb extra, integrates peft and bnb\n",
        "\n",
        "!pip install -q accelerate bitsandbytes \"transformers[torch]\" --upgrade #installs transformers with dependencies for PyTorch\n",
        "\n",
        "!pip install -q datasets==2.19.0 scikit-learn==1.4.2 pandas openpyxl #installs datasets, scikit-learn, pandas, and openpyxl\n",
        "\n",
        "print (\"\\n libraries installed\")\n",
        "print (\"\\n restart runtime\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EI5WZYrHKSOI",
        "outputId": "4248ab3b-1b71-4be3-86b5-aa6ac8b7fd07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "installing hugging face libraries for QLoRA\n",
            "\u001b[33mWARNING: peft 0.15.2 does not provide the extra 'bnb'\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.1/411.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.1/362.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m117.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\n",
            " libraries installed\n",
            "\n",
            " restart runtime\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#imports libraries and configures data and training\n",
        "\n",
        "import torch #torch is necessary for the Llama model\n",
        "import pandas as pd #will be used later to load, clean, and process data before converting to a hugginf face dataset\n",
        "import numpy as np #library for large, multi dimensional arrays and matricies as well as operations on them\n",
        "from datasets import Dataset, DatasetDict #provides effecient way to handle large datasets for machine learning, espicially natural language processing\n",
        "from sklearn.model_selection import train_test_split #from sci-kit learn library, splits dataset into training and validation\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report #these provide information on the accuracy of the model\n",
        "from transformers import ( #key components from transformers\n",
        "    AutoTokenizer, #loads correct tokenize automatically, tokenizers convert text into numerical IDs\n",
        "    AutoModelForSequenceClassification, #standard class the automatically load the correct model for sequence classification tasks\n",
        "    TrainingArguments, #class to configure training\n",
        "    Trainer, #class that simplifies training and eval loop for hugging face models\n",
        "    DataCollatorWithPadding, #pads sequences of data in a batch to be the same legnth\n",
        "    BitsAndBytesConfig # manual QLoRA config, quantization is important bc of GPU resources\n",
        ")\n",
        "from peft import (\n",
        "    get_peft_model, #wraps model with peft adapter like QLoRA\n",
        "    LoraConfig, #class for LoRA config\n",
        "    TaskType, #specifies task type\n",
        "    prepare_model_for_kbit_training # manual QLoRA setup with kbit quantization\n",
        ")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\") #makes output cleaner\n",
        "from huggingface_hub import notebook_login # Keep login for Llama 3.1\n",
        "\n",
        "CSV_PATH = \"2.4.xlsx\" #uploaded excel file\n",
        "\n",
        "# Column Names (Ensure these EXACTLY match your cleaned CSV/Excel headers)\n",
        "COL_SUBFUNCTION = \"Subfunction\"\n",
        "COL_REQUIREMENTS = \"Requirements\"\n",
        "COL_FAILURE_MODE = \"Potential Failure Mode and descriptions\" # Base name, will be cleaned\n",
        "COL_EFFECT_PRIMARY = \"Potential Effect(s) of Failure (primary)\" # Base name\n",
        "COL_EFFECT_SECONDARY = \"Potential Effect(s) of Failure (secondary)\" # Base name\n",
        "COL_SEVERITY = \"Severity\" # target column\n",
        "\n",
        "# Input/Output Columns\n",
        "INPUT_TEXT_COLS = [\n",
        "    COL_SUBFUNCTION, COL_REQUIREMENTS, COL_FAILURE_MODE,\n",
        "    COL_EFFECT_PRIMARY, COL_EFFECT_SECONDARY\n",
        "]\n",
        "\n",
        "\n",
        "#need to fill in empty cells\n",
        "COLS_TO_FORWARD_FILL = [\n",
        "    COL_SUBFUNCTION, COL_REQUIREMENTS, COL_FAILURE_MODE\n",
        "]\n",
        "\n",
        "#want to predict this\n",
        "TARGET_COLUMN = COL_SEVERITY\n",
        "NUM_LABELS = 10 # Severity can only be 1-10\n",
        "\n",
        "# Model Config\n",
        "MODEL_NAME = \"meta-llama/Llama-3.2-1B\" # using 3.2-1B bc 3.1 access has not been approved\n",
        "MAX_SEQ_LENGTH = 512 #for memory management\n",
        "\n",
        "# Training Config\n",
        "OUTPUT_DIR = \"fmea_severity_classifier_llama32_1b_standard_qlora\" # <<< New output dir name\n",
        "LEARNING_RATE = 1e-4      # common QLoRA starting point\n",
        "NUM_EPOCHS = 3            # Train for 3 epochs which is standard for fine tuning\n",
        "# MAX_STEPS = 500         # Alternative to epochs\n",
        "BATCH_SIZE_PER_DEVICE = 4 # changed to 4 because model is smaller\n",
        "GRAD_ACCUMULATION_STEPS = 4 # effective batch size of 16\n",
        "LORA_R = 16 #the rank for LoRA matricies 16 is common\n",
        "LORA_ALPHA = 32 #scaling factor for LoRA updates twice or equal to LORA_R is standard\n",
        "LORA_DROPOUT = 0.05 #used for regularization, 0.05 is typical value\n",
        "LOGGING_STEPS = 10 #frequent updates\n",
        "SAVE_STRATEGY = \"epoch\" #when model should be saved\n",
        "EVAL_STRATEGY = \"epoch\"   #when eval should be preformed\n",
        "# -----done configing LoRA------\n",
        "\n",
        "\n",
        "#setup device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #checks if device is uisng cuda GPU, cpu for backup\n",
        "print(f\"Using device: {device}\") #prints out what device is being used\n",
        "if device.type == 'cpu': print(\"Warning: Running on CPU!\") #warning for if using cpu bc cpu training is way too slow\n",
        "\n",
        "# Setup Label Mappings\n",
        "labels_list = [str(i) for i in range(1, 11)] #creates list of strings for severity label from \"1\" to \"10\"\n",
        "id2label = {i: label for i, label in enumerate(labels_list)} #maps integers 0-9 to strings \"1\"-\"10\" needed for classification head and output interpretation\n",
        "label2id = {label: i for i, label in enumerate(labels_list)} #reverse of previous line, maps integers \"1\"-\"10\" to integers 0-9 which is required for training\n",
        "print(f\"id2label mapping: {id2label}\") #print for verification\n",
        "print(f\"label2id mapping: {label2id}\") #print for verification\n",
        "\n",
        "# Check GPU capability for compute dtype in BNBConfig will  use float16 because t4 is the free avalible one\n",
        "compute_dtype = torch.float16\n",
        "if torch.cuda.is_available():\n",
        "    if torch.cuda.get_device_capability()[0] >= 8: # Ampere+ (A100)\n",
        "        compute_dtype = torch.bfloat16\n",
        "        print(\"Compute dtype set to bfloat16 for Ampere+ GPU.\")\n",
        "    else:\n",
        "        print(\"Compute dtype set to float16.\")"
      ],
      "metadata": {
        "id": "W8OzvxnGNf10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8aa19dbd-b298-4a48-cc55-ac1cf3dd0789"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "id2label mapping: {0: '1', 1: '2', 2: '3', 3: '4', 4: '5', 5: '6', 6: '7', 7: '8', 8: '9', 9: '10'}\n",
            "label2id mapping: {'1': 0, '2': 1, '3': 2, '4': 3, '5': 4, '6': 5, '7': 6, '8': 7, '9': 8, '10': 9}\n",
            "Compute dtype set to float16.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load and Preprocess Data\n",
        "#read Excel/CSV, clean header, forward filling, combine text features into a combined \"text\" column, and \"label\" column 0-9\n",
        "#splits data\n",
        "#converts to DatasetDict\n",
        "#normalization off by default\n",
        "\n",
        "print(f\"Loading data from '{CSV_PATH}'...\")\n",
        "\n",
        "\n",
        "#try except block handles reading from excel and csv, and then stores it into pandas dataframe, df\n",
        "try:\n",
        "    try: df = pd.read_excel(CSV_PATH)\n",
        "    except Exception: df = pd.read_csv(CSV_PATH)\n",
        "    #this line will perform string operations to datafram column names, removes new line characters with \\n, replaces multiple spaces with a single space\n",
        "    #removes leading and trailing whitespace\n",
        "    #saves the processed columns to cleaned_columns list\n",
        "    original_columns = df.columns.tolist(); df.columns = df.columns.str.replace('\\n', '', regex=False).str.replace(' +', ' ', regex=True).str.strip(); cleaned_columns = df.columns.tolist()\n",
        "    #maps cleaned columns to original columns\n",
        "    column_map = {clean: orig for clean, orig in zip(cleaned_columns, original_columns)}; print(f\"✅ Loaded {len(df)} rows. Cleaned columns: {cleaned_columns}\")\n",
        "except Exception as e: print(f\"❌ Error loading data: {e}\"); raise #print error if reading from excel or csv produced an exception\n",
        "\n",
        "\n",
        "# Function to get cleaned name robustly (optional, can hardcode if sure)\n",
        "def get_cleaned_name(config_name, df_cols, original_map): # Pass original map too\n",
        "    # Use split() and join() to collapse multiple spaces and remove newlines/strip\n",
        "    cleaned = ' '.join(str(config_name).replace('\\n', '').strip().split()) # same cleaning as before\n",
        "    if cleaned not in df_cols:\n",
        "         original_name = original_map.get(cleaned, config_name) # Try lookup original name if clean fails\n",
        "         print(f\"   Warning: Configured column '{original_config_name}' -> '{cleaned}' not found after cleaning. Check CSV/Excel headers and config variables.\")\n",
        "         # Fallback to original name might be safer if cleaning leads to mismatch\n",
        "         if original_name in df_cols: return original_name\n",
        "         return config_name # Return original config if neither found\n",
        "    return cleaned\n",
        "\n",
        "# Update configured names based on cleaned names IN THE DATAFRAME\n",
        "# This has all parts of the FMEA table, and uses the funciton to get cleaned names\n",
        "# ceratin cells to forward fill bc they are partialy empty in table\n",
        "COL_SUBFUNCTION = get_cleaned_name(COL_SUBFUNCTION, df.columns, column_map)\n",
        "COL_REQUIREMENTS = get_cleaned_name(COL_REQUIREMENTS, df.columns, column_map)\n",
        "COL_FAILURE_MODE = get_cleaned_name(COL_FAILURE_MODE, df.columns, column_map)\n",
        "COL_EFFECT_PRIMARY = get_cleaned_name(COL_EFFECT_PRIMARY, df.columns, column_map)\n",
        "COL_EFFECT_SECONDARY = get_cleaned_name(COL_EFFECT_SECONDARY, df.columns, column_map)\n",
        "COL_SEVERITY = get_cleaned_name(COL_SEVERITY, df.columns, column_map)\n",
        "INPUT_TEXT_COLS = [COL_SUBFUNCTION, COL_REQUIREMENTS, COL_FAILURE_MODE, COL_EFFECT_PRIMARY, COL_EFFECT_SECONDARY]\n",
        "COLS_TO_FORWARD_FILL = [COL_SUBFUNCTION, COL_REQUIREMENTS, COL_FAILURE_MODE]\n",
        "TARGET_COLUMN = COL_SEVERITY # Already potentially cleaned\n",
        "all_needed_columns = INPUT_TEXT_COLS + [TARGET_COLUMN]\n",
        "print(f\"   Using effective columns: {all_needed_columns}\")\n",
        "\n",
        "# Verify Columns Exist\n",
        "missing_cols = [col for col in all_needed_columns if col not in df.columns]; #list of columns in all_needed_columns not in df.columns, which is all columns\n",
        "if missing_cols: print(f\"❌ Error: Columns missing: {missing_cols}\"); raise ValueError(\"Missing columns\") #print out result if missing columns is not empty, and raises error\n",
        "\n",
        "# Preprocessing\n",
        "print(\"⏳ Preprocessing data...\")\n",
        "df_selected = df[all_needed_columns].copy() #only takes needed columns\n",
        "print(f\"   Forward filling columns: {COLS_TO_FORWARD_FILL}...\")\n",
        "df_selected[COLS_TO_FORWARD_FILL] = df_selected[COLS_TO_FORWARD_FILL].ffill() #forward fills the columns that need it\n",
        "initial_rows = len(df_selected); df_selected = df_selected.dropna(); final_rows = len(df_selected) #count initial rows with len(), then drops rows empty in any column, counts rows after dropping\n",
        "if initial_rows > final_rows: print(f\"   Dropped {initial_rows - final_rows} rows with NaN values.\") #prints if rows were dropped\n",
        "if final_rows == 0: raise ValueError(\"No data left after NaN drop\") #if everything was dropped, raise an Error\n",
        "\n",
        "# Convert Severity & Validate\n",
        "try:\n",
        "    df_selected[TARGET_COLUMN] = pd.to_numeric(df_selected[TARGET_COLUMN], errors='coerce') #converts values in TARGET_COLUMN into a number type, if can't turn into NaN\n",
        "    df_selected = df_selected.dropna(subset=[TARGET_COLUMN]); df_selected[TARGET_COLUMN] = df_selected[TARGET_COLUMN].astype(int) #removes rows if converted to NaN\n",
        "except Exception as e: print(f\"❌ Error converting Severity: {e}\"); raise\n",
        "#filter out data not in 1-10 range and checks if any data remains\n",
        "initial_rows = len(df_selected); df_selected = df_selected[df_selected[TARGET_COLUMN].between(1, 10)]; final_rows = len(df_selected)\n",
        "#sets df_selected to only have rows with value 1-10, then counts the new ammount of rows\n",
        "if initial_rows > final_rows: print(f\"   Removed {initial_rows - final_rows} rows with Severity outside [1, 10].\") #prints out how many rows were removed\n",
        "if final_rows == 0: raise ValueError(\"No data left with valid Severity (1-10)\")\n",
        "\n",
        "# Combine Text Features\n",
        "def combine_features(row):\n",
        "    text_parts = [] #empty list to store the text\n",
        "    for col in INPUT_TEXT_COLS: value = str(row[col]) if pd.notna(row[col]) else \"\"; clean_col_name = col.split('(')[0].strip(); text_parts.append(f\"{clean_col_name}: {value}\")\n",
        "    #goes through every input column and gets the value if it is not NaN, splits at ( and removes leading and trailing whitespace then adds it to text_parts\n",
        "    return \"\\n\".join(text_parts) #returns all text parts together with new line between them as a input for the model\n",
        "print(\"   Combining input text features into 'text' column...\")\n",
        "df_selected['text'] = df_selected.apply(combine_features, axis=1) #creates new column called text, fills it with the combined features in each row\n",
        "\n",
        "# Prepare Labels (0-9)\n",
        "df_selected['label'] = df_selected[TARGET_COLUMN] - 1 #converts from 1-10 to 0-9 because 0 is the first and populates new column label with those values\n",
        "print(f\"   Created 'label' column (0-9) from '{TARGET_COLUMN}'.\")\n",
        "\n",
        "# Keep only necessary columns\n",
        "df_final = df_selected[['text', 'label']] #just needs the combined text features and the 0-9 severity value\n",
        "\n",
        "# Create Train/Validation Split\n",
        "print(\"⏳ Splitting data...\")\n",
        "train_df, valid_df = train_test_split(df_final, test_size=0.2, random_state=42, stratify=df_final['label']) #80% for training, 20% for validation severity makes sure that severity levels are evenly distributed\n",
        "print(f\"✅ Split complete. Train size: {len(train_df)}, Validation size: {len(valid_df)}\") #prints the number of rows for training and validating\n",
        "\n",
        "# Convert to Hugging Face Datasets\n",
        "train_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n",
        "valid_dataset = Dataset.from_pandas(valid_df, preserve_index=False)\n",
        "raw_datasets = DatasetDict({'train': train_dataset, 'validation': valid_dataset}) #create DatasetDict with train and validation and corresponding datasets which is needed for Trainer\n",
        "print(\"✅ Data prepared and converted to Hugging Face Datasets format.\")\n",
        "print(raw_datasets)\n",
        "\n",
        "#  cleanup\n",
        "import gc; del df, df_selected, df_final, train_df, valid_df; gc.collect() #imports garbage collector, deletes unneeded large pandas dataframes for memory saving, and runs garbage collector for memory"
      ],
      "metadata": {
        "id": "jsvggni9DkTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Hugging Face Login---\n",
        "print(\"\\nPlease log in to Hugging Face using an Access Token with 'read' permission.\")\n",
        "notebook_login()\n",
        "print(\"✅ Login process initiated.\")\n",
        "# --- End Login ---\n",
        "print(f\"\\n⏳ Loading tokenizer for '{MODEL_NAME}'...\")\n",
        "# Load tokenizer associated with the base model\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME) # Login above handles token\n",
        "    # Set padding token (Llama 3 uses EOS)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        print(f\"   Tokenizer pad_token set to eos_token: {tokenizer.pad_token}\")\n",
        "    print(\"✅ Tokenizer loaded.\")\n",
        "except Exception as e: print(f\"❌ Error loading tokenizer: {e}\"); raise"
      ],
      "metadata": {
        "id": "fc-fpWD0-0va"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NB4THQKd6h8m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}