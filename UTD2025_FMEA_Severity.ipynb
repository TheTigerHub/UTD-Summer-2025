{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyML5Pg65U2RKm2jJ0GS/cRq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheTigerHub/UTD-Summer-2025/blob/main/UTD2025_FMEA_Severity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgwOMXjswnBk",
        "outputId": "36bd8930-54a8-498a-8d55-3b1b378f1fbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "installing hugging face libraries for QLoRA\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hinstalled accelerate\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hinstalled peft\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hinstalled bitsandbytes\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hinstalled transformers\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0minstalled datasets\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hinstalled scikit-learn\n",
            "installed pandas\n",
            "installed openpyxl\n",
            "\n",
            " libraries installed\n"
          ]
        }
      ],
      "source": [
        "print (\"installing hugging face libraries for QLoRA\")\n",
        "\n",
        "!pip install -q accelerate==0.30.1 #hardware acceleration\n",
        "print (\"installed accelerate\")\n",
        "!pip install -q peft==0.11.1 #peft is parameter effecient fine tuning which allows LoRA and QLoRA\n",
        "print (\"installed peft\")\n",
        "!pip install -q bitsandbytes==0.43.1 #bits and bytes helps with quantization\n",
        "print(\"installed bitsandbytes\")\n",
        "!pip install -q transformers==4.41.1 #hugging face library for pre-trained models, tokenizers\n",
        "print(\"installed transformers\")\n",
        "!pip install -q datasets==2.19.0 #datasets is used for loading, processing, and managing datasets\n",
        "print(\"installed datasets\")\n",
        "!pip install -q scikit-learn==1.4.2 # scikit-learn for data splitting\n",
        "print(\"installed scikit-learn\")\n",
        "!pip install -q pandas #used for data manipulation and analysis\n",
        "print(\"installed pandas\")\n",
        "!pip install -q openpyxl #read excel\n",
        "print(\"installed openpyxl\")\n",
        "\n",
        "print (\"\\n libraries installed\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"installing hugging face libraries for QLoRA\")\n",
        "\n",
        "!pip install -q \"peft[bnb]\" --upgrade #installs peft pacakge with bnb extra, integrates peft and bnb\n",
        "\n",
        "!pip install -q accelerate bitsandbytes \"transformers[torch]\" --upgrade #installs transformers with dependencies for PyTorch\n",
        "\n",
        "!pip install -q datasets==2.19.0 scikit-learn==1.4.2 pandas openpyxl #installs datasets, scikit-learn, pandas, and openpyxl\n",
        "\n",
        "print (\"\\n libraries installed\")\n",
        "print (\"\\n restart runtime\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EI5WZYrHKSOI",
        "outputId": "4248ab3b-1b71-4be3-86b5-aa6ac8b7fd07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "installing hugging face libraries for QLoRA\n",
            "\u001b[33mWARNING: peft 0.15.2 does not provide the extra 'bnb'\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.1/411.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.1/362.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m117.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\n",
            " libraries installed\n",
            "\n",
            " restart runtime\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#imports libraries and configures data and training\n",
        "\n",
        "import torch #torch is necessary for the Llama model\n",
        "import pandas as pd #will be used later to load, clean, and process data before converting to a hugginf face dataset\n",
        "import numpy as np #library for large, multi dimensional arrays and matricies as well as operations on them\n",
        "from datasets import Dataset, DatasetDict #provides effecient way to handle large datasets for machine learning, espicially natural language processing\n",
        "from sklearn.model_selection import train_test_split #from sci-kit learn library, splits dataset into training and validation\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report #these provide information on the accuracy of the model\n",
        "from transformers import ( #key components from transformers\n",
        "    AutoTokenizer, #loads correct tokenize automatically, tokenizers convert text into numerical IDs\n",
        "    AutoModelForSequenceClassification, #standard class the automatically load the correct model for sequence classification tasks\n",
        "    TrainingArguments, #class to configure training\n",
        "    Trainer, #class that simplifies training and eval loop for hugging face models\n",
        "    DataCollatorWithPadding, #pads sequences of data in a batch to be the same legnth\n",
        "    BitsAndBytesConfig # manual QLoRA config, quantization is important bc of GPU resources\n",
        ")\n",
        "from peft import (\n",
        "    get_peft_model, #wraps model with peft adapter like QLoRA\n",
        "    LoraConfig, #class for LoRA config\n",
        "    TaskType, #specifies task type\n",
        "    prepare_model_for_kbit_training # manual QLoRA setup with kbit quantization\n",
        ")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\") #makes output cleaner\n",
        "from huggingface_hub import notebook_login # Keep login for Llama 3.1\n",
        "\n",
        "CSV_PATH = \"2.4.xlsx\" #uploaded excel file\n",
        "\n",
        "# Column Names (Ensure these EXACTLY match your cleaned CSV/Excel headers)\n",
        "COL_SUBFUNCTION = \"Subfunction\"\n",
        "COL_REQUIREMENTS = \"Requirements\"\n",
        "COL_FAILURE_MODE = \"Potential Failure Mode and descriptions\" # Base name, will be cleaned\n",
        "COL_EFFECT_PRIMARY = \"Potential Effect(s) of Failure (primary)\" # Base name\n",
        "COL_EFFECT_SECONDARY = \"Potential Effect(s) of Failure (secondary)\" # Base name\n",
        "COL_SEVERITY = \"Severity\" # target column\n",
        "\n",
        "# Input/Output Columns\n",
        "INPUT_TEXT_COLS = [\n",
        "    COL_SUBFUNCTION, COL_REQUIREMENTS, COL_FAILURE_MODE,\n",
        "    COL_EFFECT_PRIMARY, COL_EFFECT_SECONDARY\n",
        "]\n",
        "\n",
        "\n",
        "#need to fill in empty cells\n",
        "COLS_TO_FORWARD_FILL = [\n",
        "    COL_SUBFUNCTION, COL_REQUIREMENTS, COL_FAILURE_MODE\n",
        "]\n",
        "\n",
        "#want to predict this\n",
        "TARGET_COLUMN = COL_SEVERITY\n",
        "NUM_LABELS = 10 # Severity can only be 1-10\n",
        "\n",
        "# Model Config\n",
        "MODEL_NAME = \"meta-llama/Llama-3.2-1B\" # using 3.2-1B bc 3.1 access has not been approved\n",
        "MAX_SEQ_LENGTH = 512 #for memory management\n",
        "\n",
        "# Training Config\n",
        "OUTPUT_DIR = \"fmea_severity_classifier_llama32_1b_standard_qlora\" # <<< New output dir name\n",
        "LEARNING_RATE = 1e-4      # common QLoRA starting point\n",
        "NUM_EPOCHS = 3            # Train for 3 epochs which is standard for fine tuning\n",
        "# MAX_STEPS = 500         # Alternative to epochs\n",
        "BATCH_SIZE_PER_DEVICE = 4 # changed to 4 because model is smaller\n",
        "GRAD_ACCUMULATION_STEPS = 4 # effective batch size of 16\n",
        "LORA_R = 16 #the rank for LoRA matricies 16 is common\n",
        "LORA_ALPHA = 32 #scaling factor for LoRA updates twice or equal to LORA_R is standard\n",
        "LORA_DROPOUT = 0.05 #used for regularization, 0.05 is typical value\n",
        "LOGGING_STEPS = 10 #frequent updates\n",
        "SAVE_STRATEGY = \"epoch\" #when model should be saved\n",
        "EVAL_STRATEGY = \"epoch\"   #when eval should be preformed\n",
        "# -----done configing LoRA------\n",
        "\n",
        "\n",
        "#setup device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #checks if device is uisng cuda GPU, cpu for backup\n",
        "print(f\"Using device: {device}\") #prints out what device is being used\n",
        "if device.type == 'cpu': print(\"Warning: Running on CPU!\") #warning for if using cpu bc cpu training is way too slow\n",
        "\n",
        "# Setup Label Mappings\n",
        "labels_list = [str(i) for i in range(1, 11)] #creates list of strings for severity label from \"1\" to \"10\"\n",
        "id2label = {i: label for i, label in enumerate(labels_list)} #maps integers 0-9 to strings \"1\"-\"10\" needed for classification head and output interpretation\n",
        "label2id = {label: i for i, label in enumerate(labels_list)} #reverse of previous line, maps integers \"1\"-\"10\" to integers 0-9 which is required for training\n",
        "print(f\"id2label mapping: {id2label}\") #print for verification\n",
        "print(f\"label2id mapping: {label2id}\") #print for verification\n",
        "\n",
        "# Check GPU capability for compute dtype in BNBConfig will  use float16 because t4 is the free avalible one\n",
        "compute_dtype = torch.float16\n",
        "if torch.cuda.is_available():\n",
        "    if torch.cuda.get_device_capability()[0] >= 8: # Ampere+ (A100)\n",
        "        compute_dtype = torch.bfloat16\n",
        "        print(\"Compute dtype set to bfloat16 for Ampere+ GPU.\")\n",
        "    else:\n",
        "        print(\"Compute dtype set to float16.\")"
      ],
      "metadata": {
        "id": "W8OzvxnGNf10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8aa19dbd-b298-4a48-cc55-ac1cf3dd0789"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "id2label mapping: {0: '1', 1: '2', 2: '3', 3: '4', 4: '5', 5: '6', 6: '7', 7: '8', 8: '9', 9: '10'}\n",
            "label2id mapping: {'1': 0, '2': 1, '3': 2, '4': 3, '5': 4, '6': 5, '7': 6, '8': 7, '9': 8, '10': 9}\n",
            "Compute dtype set to float16.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jsvggni9DkTI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}